{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H6C LST-Binning Inspection Notebook\n",
    "\n",
    "**Steven Murray & Josh Dillon**\n",
    "\n",
    "This notebook provides a sense-check for H6C LST-binned data. It can operate in two different modes: either on redundantly-averaged or non-redundantly-averaged data. Some plots will be included/omitted dependending on which of these modes is being used (the mode is auto-detected)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Figures\n",
    "\n",
    "* [Auto-Correlation Plot](#Figure:-Auto-Correlation-Plot)\n",
    "* [Mean Excess Variance as a Function of Frequency](#Figure:-Mean-Excess-Variance-as-Function-of-Frequency)\n",
    "* Distribution of Excess Variance:\n",
    "  * [As function of Days Binned](#Figure:-Distribution-of-Excess-Variance-as-function-of-Days-Binned)\n",
    "  * [Across Baseline Subsets and LSTs for Low- and High-Band](#Figure:-Distribution-of-Excess-Variance-Across-Baseline-Subsets-and-LSTs-for-Low--and-High-Band)\n",
    "  * [Across LSTs and Bands for All Baselines](#Figure:-Distribution-of-Excess-Variance-across-LSTs-and-Bands-for-All-Baselines)\n",
    "  * [As Function of Baseline Lenth and LST at 160 MHz](#Figure:-Distribution-of-Excess-Variance-with-Baseline-Length-and-LST-at-160-MHz)\n",
    "  * [Between NS and EW baselines and N/E pols](#Figure:-Distribution-of-Excess-Variance-Between-NS-and-EW-baselines-and-pols)\n",
    "  * [Across Redundant Group Size](#Figure:-Distribution-of-Excess-Variance-Across-Redundant-Group-Size)\n",
    "* [Raw Visibilities over Nights for the Worst Cases of Excess Variance](#Figure:-Visibilities-Over-Nights)\n",
    "* Distribution of Predicted Z-Scores:\n",
    "  * [For a single Frequency/LST/Night](#Figure:-Histogram-of-Baseline-Z-Scores-at-single-Frequency-/-LST-/-Night)\n",
    "  * [Per-night at 138 MHz](#Figure:-Box-Plot-of-Z-Scores-at-138-MHz)\n",
    "  * [Per-night at 169 MHz](#Figure:-Box-Plot-of-Z-Scores-at-169-MHz)\n",
    "* Sigma-Clipping\n",
    "  * [Sigma-Clipped Fraction as a Function of Threshold, LST and Night](#Figure:-Sigma-Clipped-Fraction-As-Function-of-Threshold,-LST-and-Night)\n",
    "  * [List of Most Sigma-Clipped LSTs, Nights and Baselines](#List-of-most-sigma-clipped-LSTs,-Nights-and-Baselines)\n",
    "  * [List of Most-Sigma-Clipped Antennas](#List-of-Most-Sigma-Clipped-Antennas)\n",
    "  * [List of Most-Sigma-Clipped Antenna-Nights](#List-of-Most-Sigma-Clipped-Antenna-Nights)\n",
    "  * [Counts of Contiguous Flagged Region Sizes](#Figure:-Counts-of-Contiguous-Flagged-Region-Sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T19:32:37.266279Z",
     "start_time": "2021-02-16T19:32:33.932962Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from hera_cal.io import HERAData\n",
    "from hera_cal.datacontainer import DataContainer\n",
    "import glob\n",
    "from hera_cal import utils, noise, redcal, lstbin\n",
    "from hera_cal.lstbin_simple import lst_average\n",
    "from hera_cal.abscal import match_times\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from astropy.time import Time\n",
    "from astropy import units as un\n",
    "import matplotlib as mpl\n",
    "from hera_cal import io, apply_cal\n",
    "import toml\n",
    "from hera_cal.io import HERADataFastReader\n",
    "from collections import defaultdict\n",
    "from matplotlib import patches\n",
    "from hera_opm.mf_tools import get_lstbin_datafiles\n",
    "from hera_cal.red_groups import RedundantGroups\n",
    "from hera_cal.datacontainer import RedDataContainer\n",
    "import yaml\n",
    "from pyuvdata.uvdata import FastUVH5Meta\n",
    "import re\n",
    "import json\n",
    "from functools import partial\n",
    "import copy\n",
    "import attrs\n",
    "from functools import cached_property\n",
    "from scipy.stats import gamma, norm\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the path below if running this notebook interactively\n",
    "lstbin_path = Path(os.environ.get(\n",
    "    \"LSTBIN_PATH\", \n",
    "    \"/lustre/aoc/projects/hera/h6c-analysis/IDR2/lstbin-outputs/nonavg-smoothcal/\"\n",
    "))\n",
    "\n",
    "if not lstbin_path.exists():\n",
    "    raise IOError(f\"{lstbin_path} does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the baseline groups we want to look at\n",
    "bl_groups_to_view = os.environ.get(\n",
    "    \"BLGROUPS\",  # should be in given in form '[[1, 2,\"ee\"], [0,0,\"nn\"]]'\n",
    "    [\n",
    "        (4, 7, 'ee'),\n",
    "        (4, 7, 'nn'),\n",
    "        (10, 22, 'nn'), \n",
    "        (20, 22, 'nn'), \n",
    "        (10, 47, 'nn'), \n",
    "        (81, 155, 'ee'), \n",
    "        (8, 61, 'ee'),\n",
    "        (4, 6, 'ee'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if isinstance(bl_groups_to_view, str):\n",
    "    bl_groups_to_view=[tuple(x) for x in json.loads(bl_groups_to_view)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = toml.load(lstbin_path / \"lstbin-config.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lstbin_path / 'file-config.yaml', 'r') as fl:\n",
    "    lstbin_file_config = yaml.safe_load(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlst = lstbin_file_config['config_params']['dlst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of every file that goes into the LST-bin products\n",
    "all_data_files = sum(sum(lstbin_file_config['matched_files'], start=[]), start=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the JDs of the input data files from their filenames (i.e. the first time in each file)\n",
    "data_jds = [float(re.findall(lstbin_file_config['config_params']['jd_regex'], fl)[0]) for fl in all_data_files]\n",
    "data_jd_ints = sorted({int(jd) for jd in data_jds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a simple increasing list of jd-ints that cover the full observation (including missing days)\n",
    "JDs = np.arange(int(min(data_jds)), int(max(data_jds)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dataset spans a range of days {JDs.min()} -- {JDs.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of output files containing the full dataset at a given LST, for ease of comparison.\n",
    "GOLDEN_LSTs = [float(x) for x in config['LSTBIN_OPTS']['golden_lsts'].split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_files = sorted(lstbin_path.glob('*.GOLDEN.*'))\n",
    "golden_hds = [io.HERADataFastReader(fl) for fl in golden_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = FastUVH5Meta(all_data_files[0], blts_are_rectangular=True)\n",
    "dt = (meta.times[1] - meta.times[0])*3600*24\n",
    "df = meta.freq_array[1] - meta.freq_array[0]\n",
    "dlst = (meta.lsts[1] - meta.lsts[0])%(2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that for each golden file, all the LSTs are really aligned in a single bin\n",
    "for ghd in golden_hds:\n",
    "    assert ghd.lsts.min() + dlst >= (ghd.lsts.max() - 1e-7), f\"Got range of {ghd.lsts.max()  - ghd.lsts.min()} > {dlst}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the LST-bin output file that matches the \"Golden\" LST, so we can do more easy comparison.\n",
    "# Get the file index and time index that the golden LST corresponds to\n",
    "lst_grid = lstbin_file_config['lst_grid']\n",
    "lst_grid_flat = np.array(sum(lst_grid, start=[]))\n",
    "dlst = lst_grid_flat[1] - lst_grid_flat[0]\n",
    "\n",
    "lst_edges = [(lst - dlst/2, lst + dlst/2) for lst in lst_grid_flat]\n",
    "\n",
    "# Note that the lst_edges might not all be contiguous, because some LSTs\n",
    "# might not be observed in a dataset\n",
    "            \n",
    "print(f\"Golden LSTs investigated in this notebook come from the file indices (and time indices within that file):\")\n",
    "\n",
    "golden_file_indices = {}\n",
    "golden_time_indices = {}\n",
    "new_golden_lsts = []\n",
    "for j, glst in enumerate(GOLDEN_LSTs):\n",
    "    for k, edges in enumerate(lst_edges):\n",
    "        if glst < edges[0]:\n",
    "            glst += 2*np.pi\n",
    "            \n",
    "        if glst < edges[1]:\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    lstbin_index = k\n",
    "    glst = lst_grid_flat[lstbin_index]  # this is more exact than the golden LST gotten from file name\n",
    "\n",
    "    for i, lsts in enumerate(lst_grid):\n",
    "        if glst in lsts:\n",
    "            idx = lsts.index(glst)\n",
    "            golden_file_indices[glst] = i\n",
    "            golden_time_indices[glst] = idx\n",
    "            print(f\"LST {glst*12/np.pi:6.3f} hr: file={i:>04}, time-idx={idx}\")\n",
    "            break\n",
    "\n",
    "    new_golden_lsts.append(glst)\n",
    "GOLDEN_LSTs = new_golden_lsts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put our files and hds into the same kind of dicts\n",
    "new_golden_files = {}\n",
    "new_golden_hds = {}\n",
    "for fl, hd in zip(golden_files, golden_hds):\n",
    "    lstmean = np.mean(hd.lsts)\n",
    "    idx = np.argmin(np.abs(np.array(GOLDEN_LSTs) - lstmean))\n",
    "    new_golden_files[GOLDEN_LSTs[idx]] = fl\n",
    "    new_golden_hds[GOLDEN_LSTs[idx]] = hd\n",
    "    \n",
    "golden_files = new_golden_files\n",
    "golden_hds = new_golden_hds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_meta =next(iter(golden_hds.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reds = RedundantGroups.from_antpos(golden_meta.antpos, pols=golden_meta.pols, include_autos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if this is a redundant dataset or not\n",
    "blgroups = {reds.get_ubl_key(bl) for bl in golden_meta.bls}\n",
    "RED_DATA = len(blgroups) == len(golden_meta.bls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RED_DATA:\n",
    "    print(\"This dataset is redundantly averaged\")\n",
    "else:\n",
    "    print(\"This dataset is NOT redundantly averaged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_redundant(hd, reds=None, bls=None, **kw):\n",
    "    if reds is None:\n",
    "        reds = RedundantGroups.from_antpos(hd.antpos, hd.pols, include_autos=True)\n",
    "    keyed = reds.keyed_on_bls(hd.bls)\n",
    "    if bls is not None:\n",
    "        bls = [keyed.get_ubl_key(bl) for bl in bls]\n",
    "    d, f , n = hd.read(bls=bls, **kw)\n",
    "    d = RedDataContainer(d, reds=reds)\n",
    "    f = RedDataContainer(f, reds=reds)\n",
    "    n = RedDataContainer(n, reds=reds)\n",
    "    return d, f, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_data = {}\n",
    "golden_flags = {}\n",
    "golden_nsamples = {}\n",
    "\n",
    "for glst, hd in golden_hds.items():\n",
    "    if RED_DATA:\n",
    "        # We read all the baselines, because we need the golden data to get the expected variance for each baseline.\n",
    "        golden_data[glst], golden_flags[glst], golden_nsamples[glst] = read_redundant(hd, reds=reds)\n",
    "    else:\n",
    "        # Read only autos for now\n",
    "        golden_data[glst], golden_flags[glst], golden_nsamples[glst] = hd.read(bls=[bl for bl in hd.bls if (bl[0] == bl[1] and bl[2][0] == bl[2][1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T19:33:20.255535Z",
     "start_time": "2021-02-16T19:32:37.293023Z"
    }
   },
   "outputs": [],
   "source": [
    "# load LST-binned data for the bins that match the \"GOLDEN\" LSTs\n",
    "lst_bin_files = {}\n",
    "lstbin_hds = {}\n",
    "for glst, fl_idx in golden_file_indices.items():\n",
    "    lst_edge = lst_grid[fl_idx][0] - dlst/2\n",
    "    fname = lstbin_path / \"zen.{kind}.{lst:7.5f}.sum.uvh5\".format(kind='LST', lst=lst_edge)\n",
    "    lst_bin_files[glst] = fname\n",
    "    lstbin_hds[glst] = HERADataFastReader(fname)\n",
    "\n",
    "# We read all the baselines for the lst-binned data, because we want to do averages.\n",
    "lstbin_data = {}\n",
    "lstbin_flags = {}\n",
    "lstbin_nsamples = {}\n",
    "for glst in GOLDEN_LSTs:\n",
    "    hd = lstbin_hds[glst]\n",
    "    idx = golden_time_indices[glst]\n",
    "    if RED_DATA:\n",
    "        lstbin_data[glst], lstbin_flags[glst], lstbin_nsamples[glst] = read_redundant(hd, reds=reds, times=[hd.times[idx]])\n",
    "    else:\n",
    "        # here we're careful only to read data that is not fully flagged, otherwise RAM goes through the roof.\n",
    "        _, lstbin_flags[glst], _ = hd.read(read_data=False, read_flags=True, pols=['ee', 'nn'])\n",
    "        flagged =  [bl for bl in lstbin_flags[glst].bls() if np.all(lstbin_flags[glst][bl][idx])]\n",
    "        del lstbin_flags[glst][flagged]\n",
    "        lstbin_flags[glst].select_or_expand_times([hd.times[idx]], skip_bda_check=True)\n",
    "        \n",
    "        lstbin_data[glst], _, lstbin_nsamples[glst] = hd.read(list(lstbin_flags[glst].bls()), read_nsamples=True)\n",
    "        \n",
    "        lstbin_data[glst].select_or_expand_times([hd.times[idx]], skip_bda_check=True)\n",
    "        lstbin_nsamples[glst].select_or_expand_times([hd.times[idx]], skip_bda_check=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load night-to-night standard deviations\n",
    "std_files = {glst: fl.parent / fl.name.replace('.LST.', '.STD.') for glst, fl, in lst_bin_files.items()}\n",
    "std_hds = {glst: HERADataFastReader(fl) for glst, fl in std_files.items()}\n",
    "\n",
    "std_data = {}\n",
    "std_flags = {}\n",
    "std_nsamples = {}\n",
    "for glst, hd in std_hds.items():\n",
    "    idx = golden_time_indices[glst]\n",
    "    if RED_DATA:\n",
    "        std_data[glst], std_flags[glst], std_nsamples[glst] = read_redundant(hd, reds=reds, times=[hd.times[idx]])\n",
    "    else:\n",
    "        std_data[glst], _, _ = hd.read(bls=list(lstbin_data[glst].bls()))\n",
    "        std_data[glst].select_or_expand_times([hd.times[idx]], skip_bda_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we just make double-sure that data/flags/nsamples are consistent\n",
    "for glst in GOLDEN_LSTs:\n",
    "    for bl in lstbin_data[glst].bls():\n",
    "        lstf = lstbin_flags[glst][bl]\n",
    "        lstn = lstbin_nsamples[glst][bl]\n",
    "        lstd = lstbin_data[glst][bl]\n",
    "        \n",
    "        lstf |= (lstn==0)\n",
    "        lstn[lstf] == 0\n",
    "        lstd[lstf] *= np.nan  # multiply by nan instead of setting to nan, to get the imaginary cmp.\n",
    "        std_data[glst][bl][lstf] *= np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we re-sort the \"golden lsts\" such that we get them in one contiguous chunk\n",
    "if len(GOLDEN_LSTs) < 24:\n",
    "    firstidx = np.argmax(np.diff(sorted(GOLDEN_LSTs))) + 1\n",
    "    GOLDEN_LSTs = np.roll(GOLDEN_LSTs, len(GOLDEN_LSTs) - firstidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign colors / line styles for days for the entire notebook, so we have consistency.\n",
    "styles = {}\n",
    "for i, jdint in enumerate(data_jd_ints):\n",
    "    styles[jdint] = {'color': f\"C{i%10}\", 'ls': ['-', '--', ':', '-.'][i//10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Baseline Subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some functions for obtaining different subsets of baselines (eg. long vs. short, EW vs. NS, different pols, intra- vs. inter-sector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_antenna_sectors():\n",
    "    antpos = next(iter(lstbin_data.values())).antpos\n",
    "    zero_pos = np.mean([antpos[165], antpos[166], antpos[145]], axis=0)\n",
    "    \n",
    "    sectors = {}\n",
    "    for ant, pos in antpos.items():\n",
    "        rec = pos - zero_pos\n",
    "        theta = np.arctan2(rec[1], rec[0])\n",
    "        bllen = np.sqrt(rec[0]**2 + rec[1]**2)\n",
    "        if bllen > 200:\n",
    "            sectors[ant] = 4  # outrigger\n",
    "        elif -np.pi / 3 <= theta < np.pi / 3:\n",
    "            sectors[ant] = 1\n",
    "        elif np.pi / 3 <= theta < np.pi:\n",
    "            sectors[ant] = 2\n",
    "        elif -np.pi <= theta < -np.pi/3:\n",
    "            sectors[ant] = 3\n",
    "    return sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = get_all_antenna_sectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbllen(a,b):\n",
    "    return np.sqrt(np.sum(np.square(golden_meta.antpos[a] - golden_meta.antpos[b])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ee = lambda bl: bl[2] == 'ee'\n",
    "all_nn = lambda bl: bl[2] == 'nn'\n",
    "short_bls = lambda bl: getbllen(bl[0], bl[1])<=60.0\n",
    "long_bls = lambda bl: getbllen(bl[0], bl[1])>60.0\n",
    "intersector_bls = lambda bl: sectors[bl[0]] != sectors[bl[1]]\n",
    "intrasector_bls = lambda bl: sectors[bl[0]] == sectors[bl[1]]\n",
    "\n",
    "subsets = {\n",
    "    'all': lambda bl: True,\n",
    "    'ee-only': all_ee,\n",
    "    'nn-only': all_nn,\n",
    "    'Short (<60 m) baselines': short_bls,\n",
    "    'Long (>60 m) baselines': long_bls,\n",
    "    'Inter-sector baselines': intersector_bls,\n",
    "    \"Intra-sector baselines\": intrasector_bls,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_bls(bls, days_binned, selectors=None, min_days: int=7):\n",
    "    mindaysel = lambda bl: bl in days_binned and (np.median(days_binned[bl]) >= min_days)\n",
    "    crossbl = lambda bl: bl[0] != bl[1] and bl[2][0] == bl[2][1]\n",
    "    \n",
    "    if selectors is None:\n",
    "        selectors = [mindaysel, crossbl]\n",
    "    elif callable(selectors):\n",
    "        selectors = [mindaysel, crossbl, selectors]\n",
    "    else:\n",
    "        selectors.extend([crossbl, mindaysel])\n",
    "        \n",
    "    select = lambda bl: all(sel(bl) for sel in selectors)\n",
    "    \n",
    "    return [bl for bl in bls if select(bl)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autos(\n",
    "    data: DataContainer, flags: DataContainer, tidx=slice(None), fig=None, ax=None, \n",
    "    xlabel: bool=True, ylabel: bool = True, title: bool=True, legend=True, color=None,\n",
    "    freq_step: int=1\n",
    "):\n",
    "    if fig is None:\n",
    "        fig, ax = plt.subplots(\n",
    "            1, 2, sharex=True, sharey=True, figsize=(15,7), \n",
    "            gridspec_kw={'wspace': 0.0}, constrained_layout=True\n",
    "        )\n",
    "    else:\n",
    "        assert len(ax) == 2\n",
    "    \n",
    "    handles = []\n",
    "    for i, pol in enumerate(data.pols()):\n",
    "        if pol[0] != pol[1]:\n",
    "            continue  # skip non-autos\n",
    "        \n",
    "        plt.sca(ax[i])    \n",
    "        \n",
    "        bls = [bl for bl in reds[(0,0,pol)] if bl in data]\n",
    "\n",
    "        for j, bl in enumerate(bls):\n",
    "            thisd = np.where(flags[bl][tidx], np.nan, np.abs(data[bl][tidx]))\n",
    "                \n",
    "            for k, spec in enumerate(thisd):\n",
    "                jdint = int(data.times[tidx][k])\n",
    "                \n",
    "                if i==0 and j==0:\n",
    "                    handles.append(mpl.lines.Line2D([0], [0], label=str(jdint), **styles[jdint]))\n",
    "                    \n",
    "                if np.all(np.isnan(spec)):\n",
    "                    continue \n",
    "                    \n",
    "                if color:\n",
    "                    plt.plot(\n",
    "                        data.freqs[::freq_step] / 1e6, \n",
    "                        spec[::freq_step], \n",
    "                        color=color\n",
    "                    )\n",
    "                else:\n",
    "                    plt.plot(\n",
    "                        data.freqs[::freq_step] / 1e6, \n",
    "                        spec[::freq_step], \n",
    "                        **styles[jdint]\n",
    "                    )\n",
    "                \n",
    "        plt.yscale('log')\n",
    "        if title:\n",
    "            plt.title(f\"{pol} Pol\")\n",
    "\n",
    "        if xlabel:\n",
    "            plt.xlabel(\"Frequency [MHz]\")\n",
    "    if ylabel:\n",
    "        ax[0].set_ylabel(\"|V| [Jy]\")\n",
    "    if legend:\n",
    "        ax[0].legend(loc='lower left', ncols=3, handles=handles)\n",
    "\n",
    "            \n",
    "def plot_autos_multi_lst(datas: dict[float, DataContainer], flags: dict[float, DataContainer], \n",
    "                         tidx: int=slice(None), freq_step: int = 1):\n",
    "    fig, ax = plt.subplots(\n",
    "        len(datas), 2, figsize=(10, max(1.5*len(datas), 6)), \n",
    "        sharex=True, sharey=True, \n",
    "        gridspec_kw={'hspace': 0, 'wspace': 0}, constrained_layout=True\n",
    "    )\n",
    "    \n",
    "    fig.suptitle(\"All Auto-Correlations\")\n",
    "\n",
    "    handles = []\n",
    "    for jdint in data_jd_ints:\n",
    "        handles.append(mpl.lines.Line2D([0], [0], label=str(jdint), **styles[jdint]))\n",
    "    handles.append(mpl.lines.Line2D([0], [0], label=\"LST-average\", color='k'))\n",
    "    ax[0,0].legend(handles=handles, ncols=3)\n",
    "    for i , key in enumerate(GOLDEN_LSTs):\n",
    "        plot_autos(\n",
    "            datas[key], flags[key], tidx=tidx, fig=fig, ax=ax[i], \n",
    "            xlabel=i==(len(GOLDEN_LSTs)-1), \n",
    "            title=i==0,\n",
    "            legend=False,\n",
    "            freq_step=freq_step\n",
    "        )\n",
    "        plot_autos(\n",
    "            lstbin_data[key], \n",
    "            lstbin_flags[key], fig=fig, ax=ax[i], \n",
    "            xlabel=False, title=False, \n",
    "            color='k',\n",
    "            freq_step=freq_step,\n",
    "            legend=False\n",
    "        )\n",
    "        ax[i, 1].text(\n",
    "            0.8, 0.8, \n",
    "            f\"{key*12/np.pi:5.2f} hr\", size=14, transform=ax[i,1].transAxes\n",
    "        )\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Auto-Correlation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autos_multi_lst(golden_data, golden_flags, freq_step=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1:** All autocorrelations in the night-to-night data going into the LST averages explored in this notebook. Each row is an LST bin, columns are polarizations. Colored lines represent different nights, and the black line represents the LST average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions of Excess Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we explore the distribution of the night-to-night variance and variance of LST-averaged data. We derive theoretical distributions for these quantities in Memo #XXXX. We look at how these quantities vary with frequency, LST, various properties of the baselines, and things like autocorrelation magnitude and baseline group size (where applicable). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first set up some classes to deal with the theoretical and observed variance distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rv_continuous\n",
    "\n",
    "class MixtureModel(rv_continuous):\n",
    "    \"\"\"A distribution model from mixing multiple models.\n",
    "    \n",
    "    Taken from https://stackoverflow.com/a/72315113/1467820\n",
    "    \"\"\"\n",
    "    def __init__(self, submodels, *args, weights = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.submodels = submodels\n",
    "        if weights is None:\n",
    "            weights = [1 for _ in submodels]\n",
    "        if len(weights) != len(submodels):\n",
    "            raise(ValueError(f'There are {len(submodels)} submodels and {len(weights)} weights, but they must be equal.'))\n",
    "        self.weights = [w / sum(weights) for w in weights]\n",
    "        \n",
    "    def _pdf(self, x):\n",
    "        pdf = self.submodels[0].pdf(x) * self.weights[0]\n",
    "        for submodel, weight in zip(self.submodels[1:], self.weights[1:]):\n",
    "            pdf += submodel.pdf(x)  * weight\n",
    "        return pdf\n",
    "            \n",
    "    def _sf(self, x):\n",
    "        sf = self.submodels[0].sf(x) * self.weights[0]\n",
    "        for submodel, weight in zip(self.submodels[1:], self.weights[1:]):\n",
    "            sf += submodel.sf(x)  * weight\n",
    "        return sf\n",
    "\n",
    "    def _cdf(self, x):\n",
    "        cdf = self.submodels[0].cdf(x) * self.weights[0]\n",
    "        for submodel, weight in zip(self.submodels[1:], self.weights[1:]):\n",
    "            cdf += submodel.cdf(x)  * weight\n",
    "        return cdf\n",
    "\n",
    "    def rvs(self, size):\n",
    "        submodel_choices = np.random.choice(len(self.submodels), size=size, p = self.weights)\n",
    "        submodel_samples = [submodel.rvs(size=size) for submodel in self.submodels]\n",
    "        rvs = np.choose(submodel_choices, submodel_samples)\n",
    "        return rvs\n",
    "\n",
    "@attrs.define(slots=False)\n",
    "class LSTBinStats:\n",
    "    days_binned: DataContainer  = attrs.field()\n",
    "    n2n_var_obs: DataContainer = attrs.field()\n",
    "    lstavg_var_obs: DataContainer = attrs.field()\n",
    "    lstavg_var_pred: DataContainer = attrs.field()\n",
    "    per_night_var_pred: DataContainer = attrs.field()\n",
    "        \n",
    "    @classmethod\n",
    "    def from_data(cls, *, \n",
    "        lstbin_data: DataContainer, lstbin_nsamples: DataContainer, lstbin_flags: DataContainer, \n",
    "        std_data: DataContainer, \n",
    "        data: DataContainer=None, nsamples: DataContainer=None, flags: DataContainer=None, \n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Get the observed and predicted variance metrics from observations in a particular LST bin.\"\"\"\n",
    "        days_binned = {}\n",
    "        all_obs_var = {}\n",
    "        all_predicted_var = {}\n",
    "        all_interleaved_var = {}\n",
    "        all_predicted_binned_var = {}\n",
    "        excess_binned_var = {}\n",
    "        excess_interleaved_var = {}\n",
    "        per_night_var_pred = {}\n",
    "        \n",
    "        # Make sure we output correct types\n",
    "        dcls = lstbin_data.__class__ # Either DataContainer or RedDataContainer\n",
    "        REDAVG = dcls == RedDataContainer\n",
    "        \n",
    "        if REDAVG:\n",
    "            dcls = partial(dcls, reds=data.reds)\n",
    "                \n",
    "        if REDAVG and (data is None or nsamples is None or flags is None):\n",
    "            raise ValueError(\"If data is redundantly-averaged, you must provide data, nsamples and flags\")\n",
    "            \n",
    "        for bl in lstbin_data.bls():\n",
    "            lstd = lstbin_data[bl][0]\n",
    "            lstn = lstbin_nsamples[bl][0]\n",
    "            lstf = lstbin_flags[bl][0]\n",
    "            stdd = std_data[bl][0]\n",
    "            \n",
    "            if np.all(lstf):\n",
    "                continue\n",
    "\n",
    "            splbl = utils.split_bl(bl)\n",
    "            if splbl[0] == splbl[1]:  # don't use autos\n",
    "                continue\n",
    "            \n",
    "            # Observed variances.\n",
    "            all_obs_var[bl] = np.abs(np.where(lstf, np.nan, stdd**2))\n",
    "            all_interleaved_var[bl] = noise.interleaved_noise_variance_estimate(\n",
    "                np.atleast_2d(np.where(lstf, np.nan, lstd)), kernel=[[1, -2, 1]]\n",
    "            )[0]\n",
    "            # Set first and last frequency to NaN\n",
    "            all_interleaved_var[bl][[0,-1]] = np.nan\n",
    "\n",
    "            if REDAVG:\n",
    "                # In the redundantly-averaged case we need to know the\n",
    "                # nsamples (and autos) on each night, because they all have\n",
    "                # different nsamples.\n",
    "                \n",
    "                # Ensure flagged data has zero samples\n",
    "                gd = data[bl]\n",
    "                gn = nsamples[bl].copy()\n",
    "                gf = flags[bl]\n",
    "\n",
    "                gn[gf] = 0\n",
    "\n",
    "                # This might be slighly wrong because it gets a different variance\n",
    "                # each night not just from the Nsamples but also the autos. In the \n",
    "                # sample variance calculation that goes in to the STD files, we\n",
    "                # use only the nsamples.\n",
    "                per_day_expected_var = noise.predict_noise_variance_from_autos(\n",
    "                    bl, data, dt=dt, df=df, nsamples=nsamples\n",
    "                )\n",
    "                per_day_expected_var[gf] = np.inf\n",
    "                per_night_var_pred[bl] = per_day_expected_var\n",
    "            \n",
    "                wgts_arr = np.where(gf, 0, per_day_expected_var**-1) \n",
    "\n",
    "                # compute ancillary statistics, see math above\n",
    "                days_binned[bl] = np.sum(gn > 0, axis=0)\n",
    "            \n",
    "            \n",
    "                all_predicted_binned_var[bl] = np.sum(wgts_arr, axis=0)**-1\n",
    "                all_predicted_var[bl] = (days_binned[bl] - 1) * all_predicted_binned_var[bl]\n",
    "            else:\n",
    "                # Although the above code WOULD work for non-redundantly-averaged\n",
    "                # data, it is highly inefficient, because we don't need to know \n",
    "                # the nsamples every night (since we know they're all uniform).\n",
    "                expected_var = noise.predict_noise_variance_from_autos(\n",
    "                    bl, lstbin_data, dt=dt, df=df,\n",
    "                )[0]\n",
    "                expected_var[lstf] = np.inf\n",
    "                days_binned[bl] = lstn\n",
    "                all_predicted_binned_var[bl] = expected_var / lstn\n",
    "                all_predicted_var[bl] = all_predicted_binned_var[bl] * (lstn - 1)\n",
    "                per_night_var_pred[bl] = expected_var[None, :]\n",
    "                \n",
    "            excess_binned_var[bl] = all_obs_var[bl] / all_predicted_var[bl]\n",
    "            excess_interleaved_var[bl] = all_interleaved_var[bl] / all_predicted_binned_var[bl]\n",
    "\n",
    "        return cls(\n",
    "            days_binned=dcls(days_binned),\n",
    "            n2n_var_obs = dcls(all_obs_var),\n",
    "            lstavg_var_obs= dcls(all_interleaved_var),\n",
    "            lstavg_var_pred= dcls(all_predicted_binned_var),\n",
    "            per_night_var_pred = dcls(per_night_var_pred),\n",
    "        )\n",
    "    \n",
    "    @cached_property\n",
    "    def _cls(self):\n",
    "        if isinstance(self.days_binned, RedDataContainer):\n",
    "            return partial(RedDataContainer, reds=self.days_binned.reds)\n",
    "        else:\n",
    "            return DataContainer\n",
    "        \n",
    "    @cached_property\n",
    "    def n2n_var_pred(self) -> DataContainer:\n",
    "        return self._cls({bl: self.lstavg_var_pred[bl] * (self.days_binned[bl] - 1) for bl in self.bls()})\n",
    "    \n",
    "    @cached_property\n",
    "    def n2n_excess_var(self) -> DataContainer:\n",
    "        return self._cls({bl: self.n2n_var_obs[bl] / self.n2n_var_pred[bl] for bl in self.bls()})\n",
    "\n",
    "    @cached_property\n",
    "    def lstavg_excess_var(self) -> DataContainer:\n",
    "        return self._cls({bl: self.lstavg_var_obs[bl] / self.lstavg_var_pred[bl] for bl in self.bls()})\n",
    "    \n",
    "    @classmethod\n",
    "    def n2n_excess_var_distribution(cls, ndays_binned: int):\n",
    "        return gamma(a=(ndays_binned-1)/2, scale=2/(ndays_binned-1))\n",
    "    \n",
    "    def n2n_excess_var_pred_dist(self, bls, freq_inds=slice(None), min_n: int = 1) -> rv_continuous:\n",
    "        \"\"\"Get a scipy distribution representing the theoretical distribution of excess variance.\n",
    "        \n",
    "        This will return a MixtureModel -- i.e. it will be the expected distribution of all frequencies\n",
    "        and baselines asked for (not their average).\n",
    "        \n",
    "        \"\"\"\n",
    "        if not hasattr(bls[0], \"__len__\"):\n",
    "            bls = [bls]\n",
    "            \n",
    "        all_ns = np.concatenate(tuple(self.days_binned[bl][freq_inds] for bl in bls))\n",
    "        unique_days_binned, counts = np.unique(all_ns, return_counts=True)\n",
    "        indx = np.argwhere(unique_days_binned >= min_n)[:, 0]\n",
    "        unique_days_binned = unique_days_binned[indx]\n",
    "        counts= counts[indx]\n",
    "        \n",
    "        return MixtureModel([self.n2n_excess_var_distribution(nn) for nn in unique_days_binned], weights=counts)\n",
    "\n",
    "    def n2n_excess_var_avg_pred_dist(self, bls, freq_inds=slice(None), min_n: int = 1):\n",
    "        \"\"\"Get a scipy distribution representing the theoretical distribution of averaged excess variance.\n",
    "        \n",
    "        This will return the expected distribution of the averaged excess variance for the\n",
    "        requested baselines and frequencies. Note this is NOT the excess averaged variance (i.e.\n",
    "        we're averaging the mean-one excess over the baselines/frequencies, rather than averaging\n",
    "        the observed variance and dividing by the averaged expected variance).\n",
    "        \n",
    "        This is exact for non-redundantly averaged data, and an approximation for red-avg data.\n",
    "        Gotten from https://stats.stackexchange.com/a/191912/81338\n",
    "        \"\"\"\n",
    "        if not hasattr(bls[0], \"__len__\"):\n",
    "            bls = [bls]\n",
    "\n",
    "        ndays_binned = (np.concatenate(tuple(self.days_binned[bl][freq_inds]for bl in bls)))\n",
    "        ndays_binned = ndays_binned[ndays_binned >= min_n]\n",
    "        \n",
    "        M = len(ndays_binned)\n",
    "        ksum = np.sum(M**2 / 2 / np.sum(1/(ndays_binned - 1)))\n",
    "        thetasum = 1 / ksum\n",
    "        \n",
    "        return gamma(a=ksum, scale=thetasum)\n",
    "        \n",
    "    def bls(self):\n",
    "        return self.days_binned.bls()\n",
    "    \n",
    "    def getmean(\n",
    "        self,\n",
    "        rdc: str | RedDataContainer | DataContainer, \n",
    "        bls = None,\n",
    "        min_days: int = 7\n",
    "    ):\n",
    "        if isinstance(rdc, str):\n",
    "            rdc = getattr(self, rdc)\n",
    "        if bls is None:\n",
    "            bls = self.bls()\n",
    "            \n",
    "        return np.nanmean([np.where(self.days_binned[bl] >= min_days, rdc[bl], np.nan) for bl in bls], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "for glst in GOLDEN_LSTs:\n",
    "    stats[glst] = LSTBinStats.from_data(\n",
    "        data=golden_data[glst], nsamples=golden_nsamples[glst], flags=golden_flags[glst], \n",
    "        lstbin_data=lstbin_data[glst], lstbin_nsamples=lstbin_nsamples[glst], lstbin_flags=lstbin_flags[glst], \n",
    "        std_data=std_data[glst]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T19:36:18.068992Z",
     "start_time": "2021-02-16T19:36:18.041429Z"
    }
   },
   "outputs": [],
   "source": [
    "def noise_comparison(glst, subsets: dict[str, callable], mean_of_ratios: bool = False, log: bool=False, min_days: int = 7):\n",
    "    \n",
    "    lstbin_hd = lstbin_hds[glst]\n",
    "    \n",
    "    stat = stats[glst]\n",
    "    meanvar = stat.getmean('n2n_var_pred', min_days=min_days)\n",
    "    \n",
    "    if np.all(np.isnan(meanvar)):\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(2,2, figsize=(16,8), sharex='col', gridspec_kw={'height_ratios': [2, 1]})\n",
    "    plt.subplots_adjust(hspace=.0)\n",
    "    ax=ax.flatten()\n",
    "\n",
    "    ax[0].plot(\n",
    "        lstbin_hd.freqs/1e6, \n",
    "        meanvar, \n",
    "        lw=2, \n",
    "        label='Predicted Variance from LST-Binned\\nAutocorrelations', \n",
    "        color='k'\n",
    "    )\n",
    "    ax[0].set_ylabel('Nightly Visibility Variance (Jy$^2$) ')\n",
    "    ax[0].set_title(\n",
    "        f'Visibility Variance Across Nights at {glst*12/np.pi:5.3f} Hours LST'\n",
    "        '\\n(Mean Over Unflagged Times and Baselines)'\n",
    "    )\n",
    "    if log:\n",
    "        ax[0].set_yscale('log')\n",
    "    else:\n",
    "        ax[0].set_ylim(-100, 6000)\n",
    "\n",
    "    ax[1].plot(\n",
    "        lstbin_hd.freqs/1e6, \n",
    "        stat.getmean('lstavg_var_pred', min_days=min_days), \n",
    "        lw=2,\n",
    "        label='Predicted Variance from LST-Binned\\nAutocorrelations and N$_{samples}$',\n",
    "        color='k'\n",
    "    )\n",
    "    ax[1].set_ylabel('LST-Binned Visibility Variance (Jy$^2$)')\n",
    "    ax[1].set_title(\n",
    "        f'Variance of LST-Binned Visibilities at {glst*12/np.pi:5.3f} Hours LST'\n",
    "        '\\n(Mean Over Unflagged Times and Baselines, measured by frequency-interleaving)'\n",
    "    )\n",
    "    \n",
    "    if log:\n",
    "        ax[1].set_yscale('log')\n",
    "    else:\n",
    "        ax[1].set_ylim(-10, 200)\n",
    "    \n",
    "    for i, (name, subset) in enumerate(subsets.items()):\n",
    "        bls = get_selected_bls(stat.bls(), days_binned=stat.days_binned, selectors=subset, min_days=min_days)\n",
    "        if not bls:\n",
    "            continue\n",
    "            \n",
    "        mean_obs_var = stat.getmean('n2n_var_obs', bls, min_days=min_days)\n",
    "        mean_interleaved_var = stat.getmean('lstavg_var_obs', bls, min_days=min_days)\n",
    "        \n",
    "        if mean_of_ratios:\n",
    "            mean_excess_var = stat.getmean('n2n_excess_var', bls, min_days=min_days)\n",
    "            mean_excess_lstavg_var = stat.getmean('lstavg_excess_var', bls, min_days=min_days)\n",
    "        else:\n",
    "            mean_excess_var = mean_obs_var / stat.getmean(\"n2n_var_pred\", bls, min_days=min_days)\n",
    "            mean_excess_lstavg_var = mean_interleaved_var / stat.getmean(\"lstavg_var_pred\", bls, min_days=min_days)\n",
    "                        \n",
    "        if i == 0:\n",
    "            ax[0].plot(lstbin_hd.freqs/1e6, mean_obs_var, lw=1, label=name, color=f'C{i}')\n",
    "            ax[1].plot(lstbin_hd.freqs/1e6, mean_interleaved_var, lw=1, color=f'C{i}')\n",
    "        else:\n",
    "            # Dummy plot to get a legend\n",
    "            ax[0].plot(lstbin_hd.freqs/1e6, np.nan*np.ones(len(mean_obs_var)), lw=1, label=name, color=f'C{i}')\n",
    "            \n",
    "        ax[2].plot(lstbin_hd.freqs/1e6, mean_excess_var, color=f'C{i}', lw=1)\n",
    "        favg_rat = np.nanmean(mean_excess_var)\n",
    "        ax[2].plot(lstbin_hd.freqs/1e6, np.ones_like(lstbin_hd.freqs) * favg_rat, '--', color=f'C{i}', label=f'{favg_rat:.3f}')\n",
    "    \n",
    "        ax[3].plot(lstbin_hd.freqs/1e6, mean_excess_lstavg_var, color=f'C{i}', lw=1)\n",
    "        favg_rat = np.nanmean(mean_excess_lstavg_var)\n",
    "        ax[3].plot(lstbin_hd.freqs/1e6, np.ones_like(lstbin_hd.freqs) * favg_rat, '--', color=f'C{i}', label=f'{favg_rat:.3f}')\n",
    "\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "    ax[2].set_xlabel('Frequency (MHz)')\n",
    "    ax[2].set_xlim([40,250])\n",
    "    ax[2].set_ylim([.9, 1.5 * favg_rat])\n",
    "    ax[2].set_ylabel('Observed / Predicted')\n",
    "    ax[2].legend(loc='upper right', title='Freq-Mean Ratios', ncols=3)\n",
    "\n",
    "    ax[3].set_xlabel('Frequency (MHz)')\n",
    "    ax[3].set_ylim([.9, 1.5 * favg_rat])\n",
    "    ax[3].set_xlim([40,250])\n",
    "    ax[3].set_ylabel('Observed / Predicted')\n",
    "    ax[3].legend(loc='upper right', title=\"Freq-Mean Ratios\", ncols=3)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Mean Excess Variance as Function of Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of noise predicted from autocorrelations (and $N_{samples}$) to the noise measured either from the standard deviation across nights or from frequency-interleaving.\n",
    "\n",
    "Based on [Validation Test 4.0.0b](https://github.com/HERA-Team/hera-validation/blob/master/test-series/4/test-4.0.0b.ipynb) and [Aguirre et al. (2021) Figure 12](https://www.overleaf.com/project/5e7cdde364f7d40001749218) (the H1C IDR2 Validation paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T19:36:19.665547Z",
     "start_time": "2021-02-16T19:36:18.071612Z"
    }
   },
   "outputs": [],
   "source": [
    "for glst in GOLDEN_LSTs:\n",
    "    noise_comparison(glst, subsets, log=True, min_days=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Distribution of Excess Variance as function of Days Binned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, the excess variance has a distribution that is dependent only on the number of days being binned and no other variable (eg. variance of any particular night). The following plot shows these distributions vs. the theoretical prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = stats[GOLDEN_LSTs[5]]\n",
    "all_nds = np.concatenate([stat.days_binned[bl] for bl in stat.bls()])\n",
    "unique_nds = np.sort(np.unique(all_nds))\n",
    "unique_nds = unique_nds[unique_nds > 2]\n",
    "\n",
    "fig, ax = plt.subplots(len(unique_nds), 2, sharex=True, sharey=True,\n",
    "                       gridspec_kw={\"hspace\": 0.0, \"wspace\": 0}, figsize=(15, len(unique_nds)*2))\n",
    "\n",
    "x = np.linspace(0, 5, 200)\n",
    "for i, nd in enumerate(unique_nds):\n",
    "    for j, (name, select) in enumerate(subsets.items()):\n",
    "        bls = get_selected_bls(list(stat.bls()), stat.days_binned, selectors=select, min_days=0)\n",
    "\n",
    "        excess_low = np.concatenate([stat.n2n_excess_var[bl][(stat.days_binned[bl] == nd) & (golden_meta.freqs < 90e6)] for bl in bls])\n",
    "        excess_high = np.concatenate([stat.n2n_excess_var[bl][(stat.days_binned[bl] == nd) & (golden_meta.freqs > 110e6)] for bl in bls])\n",
    "\n",
    "        ax[i, 0].hist(excess_low, bins=np.linspace(0, 5, 101), histtype='step', density=True, color=f'C{j}', label=name)\n",
    "        ax[i, 1].hist(excess_high, bins=np.linspace(0, 5, 101), histtype='step', density=True, color=f'C{j}', label=name)\n",
    "        \n",
    "    ax[i, 0].plot(x, gamma(a=(nd-1)/2, scale=2/(nd-1)).pdf(x), color='k')\n",
    "    ax[i, 1].plot(x, gamma(a=(nd-1)/2, scale=2/(nd-1)).pdf(x), color='k')\n",
    "    \n",
    "    ax[i, 0].set_ylabel(f\"{int(nd)} days binned\")\n",
    "ax[0,0].legend(ncols=3)\n",
    "ax[0,0].set_title(\"Low Band (<90 MHz)\")\n",
    "ax[0,1].set_title(\"High Band (>110 MHz)\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_violin_plot(stat, bl_lists, coords, freq_masks=(slice(None),), min_days=7, xlabel=None, fig=None, ax=None, ylabel=True, topticks=False):\n",
    "    if fig is None:\n",
    "        fig, ax = plt.sublots(1, 1, figsize=(12,8))\n",
    "      \n",
    "    plt.sca(ax)\n",
    "    \n",
    "    # Expand freq_masks out to the length of bl_lists\n",
    "    if len(freq_masks) == 1 and len(bl_lists) > 1:\n",
    "        freq_masks = freq_masks * len(bl_lists)\n",
    "    elif len(bl_lists) == 1 and len(freq_masks) > 1:\n",
    "        bl_lists = bl_lists * len(freq_masks)\n",
    "    elif len(bl_lists) != len(freq_masks):\n",
    "        raise ValueError(\"bl_lists and freq_masks must be of the same length\")\n",
    "        \n",
    "    evs = []\n",
    "    dists = []\n",
    "    for bls, freq in zip(bl_lists, freq_masks):\n",
    "        if not bls:\n",
    "            evs.append(None)\n",
    "            dists.append(None)\n",
    "            continue\n",
    "        evs.append(np.concatenate([stat.n2n_excess_var[bl][stat.days_binned[bl] >= min_days][freq] for bl in bls]))\n",
    "        try:\n",
    "            dists.append(stat.n2n_excess_var_pred_dist(bls=bls, freq_inds=freq, min_n=min_days).rvs(size=5000))\n",
    "        except ValueError:\n",
    "            # If you get no samples, dist won't work here.\n",
    "            dists.append(None)\n",
    "            evs[-1] = None\n",
    "            \n",
    "    # Create labels for categorical-type coords based on the original number of\n",
    "    # coords, even if they end up not being shown because they have no baselines.\n",
    "    labels = None\n",
    "    if isinstance(coords[0], str):\n",
    "        labels = coords\n",
    "        coords = np.arange(len(coords))\n",
    "        plt.gca().set_xticks(np.arange(len(coords)), labels=labels)\n",
    "\n",
    "        \n",
    "    if topticks:\n",
    "        ax.xaxis.set_tick_params(labeltop=True)\n",
    "        \n",
    "    # remove all the coordinates that have no values at all\n",
    "    coords = [c for c, ev in zip(coords, evs) if ev is not None and not np.all(np.isnan(ev))]\n",
    "    dists = [d for d, ev in zip(dists, evs) if d is not None and not np.all(np.isnan(ev))]\n",
    "    evs = [ev for ev in evs if ev is not None and not np.all(np.isnan(ev))]\n",
    "    \n",
    "    if not evs:\n",
    "        return evs\n",
    "    \n",
    "    # Remove huge outliers from evs because otherwise the KDE struggles...\n",
    "    evs = [ev[ev < 10] for ev in evs]\n",
    "        \n",
    "    widths=[0.5 * (y-x) for x,y in zip(coords, coords[1:])] + [0.5 * (coords[-1] - coords[-2])]\n",
    "    parts = plt.violinplot(\n",
    "        dists, coords, showextrema=False, \n",
    "        widths=widths\n",
    "    )\n",
    "    plt.axhline(1, color='k', ls='--')\n",
    "    \n",
    "    for pc in parts['bodies']:\n",
    "        pc.set_facecolor('black')\n",
    "        pc.set_alpha(0.25)\n",
    "        \n",
    "    plt.violinplot(evs, coords, showextrema=False, showmeans=True, widths=widths)\n",
    "    \n",
    "    plt.legend(\n",
    "        handles=[\n",
    "            plt.Rectangle( (0, 0), 1, 1, facecolor='C1'), \n",
    "            plt.Rectangle((0,0), 1,1, facecolor='black', alpha=0.25)\n",
    "        ],\n",
    "        labels=['Theory', 'Data'],\n",
    "        ncols=2\n",
    "    )\n",
    "    \n",
    "    plt.ylim(0, 5)\n",
    "    if ylabel:\n",
    "        plt.ylabel(\"Excess Variance\")\n",
    "    \n",
    "    if xlabel and labels is not None:\n",
    "        plt.xlabel(xlabel)\n",
    "        \n",
    "    return evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_violin_plot(selectors, fig=None, ax=None, min_days=7, suptitle=None, ylabel=True):\n",
    "    if fig is None:\n",
    "        fig, ax = plt.subplots(len(stats), 1, sharex=True, sharey=True, figsize=(15, 2*len(stats)), constrained_layout=True)\n",
    "\n",
    "    for i, (glst, stat) in enumerate(stats.items()):\n",
    "        bls = [\n",
    "            get_selected_bls(list(stat.bls()), stat.days_binned, selectors=selector[0], min_days=min_days) for selector in selectors.values()\n",
    "        ]\n",
    "        freqmask = [selector[1] for selector in selectors.values()]\n",
    "        make_violin_plot(\n",
    "            stat, bls, \n",
    "            list(selectors.keys()), \n",
    "            fig=fig, ax=ax[i], \n",
    "            freq_masks=freqmask, \n",
    "            ylabel=False, \n",
    "            topticks=i==0\n",
    "        )\n",
    "        if ylabel:\n",
    "            ax[i].set_ylabel(f\"{glst*12/np.pi:.2f} hr\")\n",
    "\n",
    "    if suptitle:\n",
    "        fig.suptitle(suptitle)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Distribution of Excess Variance Across Baseline Subsets and LSTs for Low- and High-Band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, the orange violins represent the observed distribution of excess variance (and the horizontal orange line is the mean of each), while the gray represents the theoretical distribution for that category. The data in each violin come from all baselines within the baseline subset, and all frequencies within the specified band (there is no averaging being done, we're just taking each Baseline/LST/freq as its own datum). Only data that has at least 7 contributing days in the LST-average are counted. Low/High band refer to below and above FM respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(stats), 2, sharex=True, sharey=True, figsize=(15, 2*len(stats)), constrained_layout=True)\n",
    "\n",
    "make_full_violin_plot(\n",
    "    selectors = {name.replace(\"baselines\", \"\"): (sel, slice(None, 850)) for name, sel in subsets.items()},\n",
    "    fig=fig, ax = ax[:, 0],\n",
    "    suptitle=\"Distribution of Excess Variance across Subsets, LSTs and Bands\"\n",
    ")\n",
    "make_full_violin_plot(\n",
    "    selectors = {name.replace(\"baselines\", \"\"): (sel, slice(850, None)) for name, sel in subsets.items()},\n",
    "    fig=fig, ax = ax[:, 1], ylabel=False\n",
    ")\n",
    "\n",
    "ax[0,0].set_title(\"Low Band (< 90 MHz)\")\n",
    "ax[0,1].set_title(\"High Band (> 110 MHz)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Distribution of Excess Variance across LSTs and Bands for All Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, the orange violins represent the observed distribution of excess variance (and the horizontal orange line is the mean of each), while the gray represents the theoretical distribution for that category. The data in each violin come from all baselines, and all frequencies within the specified band of 200 channels each (there is no averaging being done, we're just taking each Baseline/LST/freq as its own datum). Only data that has at least 7 contributing days in the LST-average are counted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = make_full_violin_plot(\n",
    "    selectors = {golden_meta.freqs[ind] / 1e6: (lambda bl: True, slice(ind-100, ind+100)) for ind in range(100, 1535, 200)},\n",
    "    suptitle=\"Distribution of Excess Variance across LSTs and Bands\"\n",
    ")\n",
    "ax[-1].set_xlabel(\"Freq [MHz]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Distribution of Excess Variance with Baseline Length and LST at 160 MHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, the orange violins represent the observed distribution of excess variance (and the horizontal orange line is the mean of each), while the gray represents the theoretical distribution for that category. The data in each violin come from baselines within a given range of lengths (each bin is 14.6 m wide), and all frequencies within a 200-channel frequency band centered around 160 MHz. The choice of frequency range is intended to capture the best quality data in the spectrum. No averaging is done, we're just taking each Baseline/LST/freq as its own datum. Only data that has at least 7 contributing days in the LST-average are counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bllen_grid = [(start, start + 14.6) for start in np.arange(7.0, 180.0, 14.6)]\n",
    "\n",
    "fig, ax = make_full_violin_plot(\n",
    "    selectors = {(edge[0] + edge[1])/2: (lambda bl, edge=edge: edge[0] <= getbllen(bl[0], bl[1]) < edge[1], slice(800, 1000)) for edge in bllen_grid},\n",
    "    suptitle=\"Distribution of Excess Variance across Baseline Lengths at ~160 MHz\"\n",
    ")\n",
    "ax[-1].set_xlabel(\"Baseline Length [m]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Distribution of Excess Variance Between NS and EW baselines and pols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, the orange violins represent the observed distribution of excess variance (and the horizontal orange line is the mean of each), while the gray represents the theoretical distribution for that category. \n",
    "The data in each violin come from baselines that are North-South or East-West oriented (within 6 degrees), and further subdivided by their polarization. Data in each category is taken from all frequencies within a 200-channel frequency band centered around 160 MHz. The choice of frequency range is intended to capture the best quality data in the spectrum. No averaging is done, we're just taking each Baseline/LST/freq as its own datum. Only data that has at least 7 contributing days in the LST-average are counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bllen_grid = [(start, start + 14.6) for start in np.arange(7.0, 180.0, 14.6)]\n",
    "\n",
    "fig, ax = make_full_violin_plot(\n",
    "    selectors = {\n",
    "        \"EW (ee)\": (lambda bl: np.abs(bl[1]/bl[0]) < 1/10. and bl[2]=='ee', slice(800,1000)),\n",
    "        \"EW (nn)\": (lambda bl: np.abs(bl[1]/bl[0]) < 1/10. and bl[2]=='nn', slice(800,1000)),\n",
    "        \"NS (ee)\": (lambda bl: np.abs(bl[0]/bl[1]) < 1/10. and bl[2]=='ee', slice(800,1000)),\n",
    "        \"NS (nn)\": (lambda bl: np.abs(bl[0]/bl[1]) < 1/10. and bl[2]=='nn', slice(800,1000)),\n",
    "    },\n",
    "    suptitle=\"Distribution of Excess Variance for EW vs NS Baselines at ~160 MHz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Distribution of Excess Variance Across Redundant Group Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, the orange violins represent the observed distribution of excess variance (and the horizontal orange line is the mean of each), while the gray represents the theoretical distribution for that category. \n",
    "The data in each violin come from baselines that have redundant groups within the specified size range. Data in each category is taken from all frequencies within a 200-channel frequency band centered around 160 MHz. The choice of frequency range is intended to capture the best quality data in the spectrum. No averaging is done, we're just taking each Baseline/LST/freq as its own datum. Only data that has at least 7 contributing days in the LST-average are counted. **Note:** redgroup size is highly correlated with baseline length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size_bins = [(1, 10), (10, 20), (20, 40), (40, 80), (80, 200), (200, 500), (500, 1000), (1000, np.inf)]\n",
    "\n",
    "fig, ax = make_full_violin_plot(\n",
    "    selectors = {\n",
    "        f\"{g[0]}-{g[1]}\": (lambda bl, g=g: (g[0] <= len(reds[bl]) < g[1]), slice(800, 1000)) for g in group_size_bins\n",
    "    },\n",
    "    suptitle=\"Distribution of Excess Variance across Redundant Group Size\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Visibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we plot some raw (calibrated) data in comparison to the LST-binned data, using the \"Golden Data\" output by the LST-binner. We focus on the baselines with the highest excess variance, so that we can more easily identify issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad(x, axis=0):\n",
    "    med = np.nanmedian(x, axis=axis)\n",
    "    return np.nanmedian(np.abs(x - med), axis=axis)*1.4826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bl_coords(bl):\n",
    "    sep_unit = np.abs(golden_meta.antpos[1][0] - golden_meta.antpos[0][0])\n",
    "    return (np.abs(golden_meta.antpos[bl[0]] - golden_meta.antpos[bl[1]]) / sep_unit)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visibilities_per_type(\n",
    "    types, \n",
    "    glst,\n",
    "    freq_range=None, \n",
    "    label=None, yrange=None,\n",
    "    alpha=0.5,\n",
    "):\n",
    "    all_figs = []\n",
    "    \n",
    "    lstbin_hd = lstbin_hds[glst]\n",
    "    lststyle = dict(color='k', lw=3, alpha=0.2, zorder=10000)\n",
    "    excess = stats[glst].n2n_excess_var\n",
    "    \n",
    "    for bltype in types:\n",
    "        fig, ax = plt.subplots(\n",
    "            4, 2, \n",
    "            sharex=True, figsize=(15, 8), \n",
    "            constrained_layout=True, gridspec_kw={'height_ratios': (2,1,2,1)}\n",
    "        )\n",
    "        \n",
    "        nights_in = set()\n",
    "        if freq_range is not None:\n",
    "            mask = (lstbin_hd.freqs >= freq_range[0]) & (lstbin_hd.freqs < freq_range[1])\n",
    "            freqs=lstbin_hd.freqs[mask]/1e6\n",
    "        else:\n",
    "            mask = slice(None, None, None)\n",
    "            freqs = lstbin_hd.freqs/1e6\n",
    "            \n",
    "        \n",
    "        bls = [bl for bl in reds[bltype] if bl in golden_data[glst].bls()]\n",
    "        \n",
    "        handles = []\n",
    "        for jdint in data_jd_ints:\n",
    "            handles.append(mpl.lines.Line2D([0], [0], label=str(jdint), alpha=alpha, **styles[jdint]))\n",
    "        \n",
    "        \n",
    "        for j, bl in enumerate(bls):\n",
    "            flgs = golden_flags[glst][bl][:, mask]\n",
    "            datas = golden_data[glst][bl][:, mask]\n",
    "            \n",
    "            mag = np.where(flgs, np.nan, np.abs(datas))\n",
    "            phs = np.where(flgs, np.nan, np.angle(datas))\n",
    "            rl = np.where(flgs, np.nan, datas.real)\n",
    "            im = np.where(flgs, np.nan, datas.imag)\n",
    "\n",
    "            lstflg = lstbin_flags[glst][bl][0, mask]\n",
    "            lstdata = lstbin_data[glst][bl][0, mask]\n",
    "\n",
    "            rlmad = mad(rl)\n",
    "            immad = mad(im)\n",
    "\n",
    "            maglstbin = np.where(lstflg, np.nan, np.abs(lstdata))\n",
    "            phslstbin = np.where(lstflg, np.nan, np.angle(lstdata))\n",
    "            rllstbin = np.where(lstflg, np.nan, lstdata.real)\n",
    "            imlstbin = np.where(lstflg, np.nan, lstdata.imag)\n",
    "            \n",
    "            for night in range(len(golden_data[glst].times)):\n",
    "                jdint = int(golden_data[glst].times[night])\n",
    "                style = copy.deepcopy(styles[jdint])\n",
    "                style['alpha'] = alpha\n",
    "\n",
    "                if np.all(flgs[night]):\n",
    "                    continue\n",
    "                    \n",
    "                # Amplitude and Phase\n",
    "                ax[0, 0].plot(freqs, mag[night], **style)\n",
    "                ax[0, 0].plot(freqs, maglstbin, **lststyle)                \n",
    "                ax[1, 0].plot(freqs, mag[night] - maglstbin, **style)\n",
    "                \n",
    "                ax[2, 0].plot(freqs, phs[night], **style)\n",
    "                ax[2, 0].plot(freqs, phslstbin, **lststyle)\n",
    "                phsdiff = phs[night] - phslstbin\n",
    "                phsdiff[phsdiff < -np.pi] += 2*np.pi\n",
    "                phsdiff[phsdiff > np.pi] -= 2*np.pi\n",
    "                ax[3, 0].plot(freqs, phsdiff, **style)\n",
    "                        \n",
    "                # Real / Imag\n",
    "                ax[0, 1].plot(freqs, rl[night], **style)\n",
    "                ax[0, 1].plot(freqs, rllstbin, **lststyle)                \n",
    "                ax[1, 1].plot(freqs, (rl[night] - rllstbin)/rlmad, **style)\n",
    "                \n",
    "                ax[2, 1].plot(freqs, im[night], **style)\n",
    "                ax[2, 1].plot(freqs, imlstbin, **lststyle)\n",
    "                ax[3, 1].plot(freqs, (im[night] - imlstbin)/immad, **style)\n",
    "                \n",
    "                if yrange:\n",
    "                    ax[0, 0].set_ylim(yrange)\n",
    "                    \n",
    "            ax[1,1].axhline(4, color='gray', ls='--')\n",
    "            ax[1,1].axhline(-4, color='gray', ls='--')\n",
    "            \n",
    "            ax[3,1].axhline(4, color='gray', ls='--')\n",
    "            ax[3,1].axhline(-4, color='gray', ls='--')\n",
    "            \n",
    "        bl_coords = get_bl_coords(bltype)\n",
    "        \n",
    "        fig.suptitle(\n",
    "            f\"Baseline Type: {bltype} [{bl_coords[0]:.1f}-EW, {bl_coords[1]:.1f}-NS]. Size {len(bls)}. \"\n",
    "            f\"LST = {glst*12/np.pi:5.3} hr. Median Excess Var = {np.nanmedian(excess[bltype][mask]):.2f}\"\n",
    "        )\n",
    "        ax[-1, 0].set_xlabel(\"Frequency [MHz]\")\n",
    "        ax[-1, 1].set_xlabel(\"Frequency [MHz]\")\n",
    "        \n",
    "        ax[0, 0].set_ylabel(\"Magnitude\")\n",
    "        ax[0, 1].set_ylabel(\"Real Part\")\n",
    "        \n",
    "        ax[1, 0].set_ylabel(\"Magnitude Diff\")\n",
    "        ax[1, 1].set_ylabel(\"Real Z-score\")\n",
    "        ax[1, 1].set_ylim(-7, 7)\n",
    "        \n",
    "        ax[2, 0].set_ylabel(\"Phase\")\n",
    "        ax[2, 1].set_ylabel(\"Imag Part\")\n",
    "        \n",
    "        ax[3, 0].set_ylabel(\"Phase Diff\")\n",
    "        ax[3, 1].set_ylabel(\"Imag Z-score\")\n",
    "        ax[3, 1].set_ylim(-7, 7)\n",
    "        ax[0, 0].legend(handles=handles, ncols=5)\n",
    "\n",
    "        all_figs.append(fig)\n",
    "        \n",
    "    return all_figs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Visibilities Over Nights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_keys(stat, min_days=7):\n",
    "    excess = []\n",
    "    mask = (golden_meta.freqs>125e6) & (golden_meta.freqs<=230e6)\n",
    "    bls = []\n",
    "    for bl in stat.bls():\n",
    "        if np.mean(stat.days_binned[bl][mask]) < min_days:\n",
    "            continue\n",
    "            \n",
    "        median = np.nanmedian(stat.n2n_excess_var[bl][mask])\n",
    "        if not np.isnan(median):\n",
    "            excess.append(median)\n",
    "            bls.append(bl)\n",
    "    srt = [k for k, v in sorted(zip(bls, excess), key=lambda item: item[1])]\n",
    "    return srt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for glst in GOLDEN_LSTs:\n",
    "    # Sort keys from best to worst\n",
    "    keys = get_sorted_keys(stats[glst])\n",
    "    if keys:\n",
    "        # Three worst, and single best.\n",
    "        if RED_DATA:\n",
    "            keys = keys[-1:-4:-1] + keys[:1]\n",
    "        \n",
    "        else:\n",
    "            # We want keys from DIFFERENT red groups\n",
    "            use_bls = []\n",
    "            use_keys = []\n",
    "            for key in reversed(keys):\n",
    "                if key not in use_bls:\n",
    "                    use_keys.append(key)\n",
    "                    use_bls.extend(reds[key])\n",
    "                if len(use_keys) == 4:\n",
    "                    break\n",
    "            use_bls.extend(reds[keys[0]])\n",
    "            use_keys.append(keys[0])\n",
    "            \n",
    "            hd = golden_hds[glst]\n",
    "            use_bls = [k for k in use_bls if (k in hd.bls or utils.reverse_bl(k) in hd.bls) and (k in lstbin_flags[glst])]\n",
    "            use_bls = [k if k in hd.bls else utils.reverse_bl(k) for k in use_bls]\n",
    "            \n",
    "            # Read these particular baselines into golden_data.\n",
    "            golden_data[glst], golden_flags[glst], _ = hd.read(bls=use_bls, read_flags=True)\n",
    "            keys = use_keys\n",
    "            \n",
    "        figs = plot_visibilities_per_type(\n",
    "            keys, glst, freq_range=(125e6, 230e6), alpha=0.75,\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Predicted Z-Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_zscores(freq_inds, stat, golden_data, golden_nsamples, golden_flags, lstbin_data):\n",
    "    zscores = {}\n",
    "            \n",
    "    if len(golden_data.freqs) == len(freq_inds):\n",
    "        golden_freq_inds = list(range(len(golden_data.freqs)))\n",
    "    else:\n",
    "        golden_freq_inds = freq_inds\n",
    "        \n",
    "    for bl in golden_data.bls():\n",
    "        gd = golden_data[bl]\n",
    "        gf = golden_flags[bl]\n",
    "        \n",
    "        if bl[0] == bl[1] or bl[2][0] != bl[2][1]:\n",
    "            # skip autos\n",
    "            continue\n",
    "\n",
    "        if bl not in stat.per_night_var_pred:\n",
    "            continue\n",
    "            \n",
    "        pred_var = stat.per_night_var_pred[bl][:, freq_inds]\n",
    "        \n",
    "        zscores[bl] = np.sqrt(2) * (gd[:, golden_freq_inds] - lstbin_data[bl][0, freq_inds]) / np.sqrt(pred_var)\n",
    "        zscores[bl][gf[:, golden_freq_inds]] *= np.nan\n",
    "        \n",
    "    return RedDataContainer(zscores, reds=reds) if RED_DATA else DataContainer(zscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfreq_hds = {glst: HERADataFastReader(str(lstbin_hds[glst].filepaths[0]).replace(\".LST.\", \".REDUCEDCHAN.\")) for glst in GOLDEN_LSTs}\n",
    "view_indices = [int(x) for x in config['LSTBIN_OPTS']['save_channels'].split(\",\")]\n",
    "\n",
    "if not RED_DATA:\n",
    "    # Replace the GOLDEN data with the reduced-chan data for now. \n",
    "    for glst, hd in rfreq_hds.items():\n",
    "        golden_data[glst], golden_flags[glst], _ = hd.read(read_flags=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscores_pred = {\n",
    "    glst: get_baseline_zscores(\n",
    "        view_indices, stats[glst], golden_data[glst], golden_nsamples[glst], golden_flags[glst], lstbin_data[glst]\n",
    "    ) for glst in GOLDEN_LSTs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zscore_histogram(freq_index, glst, fig=None, ax=None, xlabel=True, legend=True):\n",
    "    if fig is None:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True, constrained_layout=True)\n",
    "    \n",
    "    bins=np.linspace(-10, 10, 101)\n",
    "    \n",
    "    for i, jd in enumerate(golden_data[glst].times):\n",
    "        this = np.array([zscores_pred[glst][bl][i, freq_index] for bl in zscores_pred[glst].bls()])\n",
    "        ax[0].hist(this.real, bins=bins, label=str(int(jd)), histtype='step', density=True, **styles[int(jd)])\n",
    "        ax[1].hist(this.imag, bins=bins, label=str(int(jd)), histtype='step', density=True, **styles[int(jd)])\n",
    "        \n",
    "    if xlabel:\n",
    "        ax[0].set_xlabel(f\"Real Z-Score at {golden_meta.freqs[freq_index]/1e6:.1f} MHz\")\n",
    "        ax[1].set_xlabel(f\"Imag Z-Score at {golden_meta.freqs[freq_index]/1e6:.1f} MHz\")\n",
    "    \n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    y = np.exp(-(x**2)/2) / np.sqrt(2*np.pi)\n",
    "    ax[0].plot(x, y, color='k', lw=3)\n",
    "    ax[1].plot(x, y, color='k', lw=3)\n",
    "    \n",
    "    if legend:\n",
    "        ax[0].legend(ncol=3, title='Night (JD)');\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[1].set_yscale('log')\n",
    "    ax[0].set_ylim(8e-4, 1)\n",
    "    ax[1].set_ylim(8e-4, 1)\n",
    "    \n",
    "def plot_all_zscore_histograms(freq_index):\n",
    "    fig, ax = plt.subplots(len(GOLDEN_LSTs), 2, figsize=(15, 2*len(GOLDEN_LSTs)), sharex=True, sharey=True, constrained_layout=True)\n",
    "    fig.suptitle(f\"Z-Scores (from predicted variance) across baselines per-night at {golden_meta.freqs[view_indices[freq_index]]/1e6:.1f} MHz \")\n",
    "    \n",
    "    ax[0,0].set_title(\"Real Part\")\n",
    "    ax[0,1].set_title(\"Imag Part\")\n",
    "    \n",
    "    handles = {}\n",
    "    for i, glst in enumerate(GOLDEN_LSTs):\n",
    "        plot_zscore_histogram(freq_index, glst, fig=fig, ax=ax[i], xlabel=i==len(GOLDEN_LSTs)-1, legend=False)\n",
    "        \n",
    "        # Keep track of legend stuff.\n",
    "        h, l = ax[i, 0].get_legend_handles_labels()\n",
    "        handles.update(dict(zip(l, h)))\n",
    "                \n",
    "        ax[i, 0].text(0.8, 0.8, f\"LST {glst*12/np.pi:5.3f} hr\", transform=ax[i,0].transAxes, fontweight='bold')\n",
    "       \n",
    "    handles = [h for l, h in sorted(handles.items())]\n",
    "    fig.legend(loc='upper left', handles=handles, ncols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Histogram of Baseline Z-Scores at single Frequency / LST / Night"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is showing the the distribution of Z-scores (with respect to the _predicted_ variance) over baselines for each night. Now, remember that the value for a *particular baseline* over nights is by definition mean-zero here (since the z-score for a baseline is defined as the visibility minus the mean over nights for that vis, divided by the std over nights), so the full distribution of everything in this plot should be mean zero, as we can see it is. However, any particular night is free to have a non-zero mean -- all baselines could have been \"bad\" on that night together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(view_indices)):\n",
    "    plot_all_zscore_histograms(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_box_plot(freq_index, glst):\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    z = zscores_pred[glst]\n",
    "    zscore_arr = [np.concatenate([z[bl][i, freq_index][~np.isnan(z[bl][i, freq_index])].real for bl in z.bls()]) for i in range(z.shape[0])]\n",
    "    jdsubs = [int(jd) - 2459800 for jd in golden_data[glst].times]\n",
    "    plt.boxplot(\n",
    "        zscore_arr, \n",
    "        positions=jdsubs\n",
    "    )\n",
    "    mean_var = np.array([np.nanmean(zscore**2) for zscore in zscore_arr])\n",
    "    plt.scatter(jdsubs, mean_var, marker='*', s=75, color='r', zorder=5, \n",
    "                label=r'$\\langle z^2 \\rangle \\approx$  Excess Variance ')\n",
    "    plt.ylim(-12, 12)\n",
    "    plt.xlabel(\"Night (JD)\")\n",
    "    plt.ylabel(\"Predicted Z-Score\")\n",
    "    \n",
    "    # Put on lines where box-plot markers should be if it were Gaussian\n",
    "    plt.axhline(0.6754, ls='--', color='gray')\n",
    "    plt.axhline(-0.6754, ls='--', color='gray')\n",
    "    plt.axhline(2.698, ls='--', color='gray', alpha=0.5)\n",
    "    plt.axhline(-2.698, ls='--', color='gray', alpha=0.5)\n",
    "    plt.axhline(1, ls='-', color='r', lw=1)\n",
    "    \n",
    "    plt.title(f\"Z-Scores Per-Night Across Baselines at {golden_meta.freqs[view_indices[freq_index]]/1e6:.1f} MHz and LST {glst*12/np.pi:5.3f} hr\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Box-Plot of Z-Scores at 138 MHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the distribution of Z-scores at 138 MHz, grouped by night, to highlight which nights (if any) are behaving poorly at each LST. Gray lines show the theoretical expectation for the box and whiskers respectively of the box plots. The red line is at unity and red stars indicate the estimate of the contribution to excess variance from that night (determined by the average of the squared z-score over baselines for that night). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for glst in GOLDEN_LSTs:\n",
    "    make_box_plot(1, glst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Box-Plot of Z-Scores at 200 MHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the distribution of Z-scores at 169 MHz (nominally a well-behaved frequency), grouped by night, to highlight which nights (if any) are behaving poorly at each LST. Gray lines show the theoretical expectation for the box and whiskers respectively of the box plots. The red line is at unity and red stars indicate the estimate of the contribution to excess variance from that night (determined by the average of the squared z-score over baselines for that night). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for glst in GOLDEN_LSTs:\n",
    "    make_box_plot(2, glst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Sigma-Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we attempt to understand the impact of sigma-clipping on the data. We form robust Z-scores from the GOLDEN data using median absolute deviation, just as in the pipeline code itself, then threshold at different thresholds to inform us of the impact of sigma-clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observed_zscores(glst, chunk_size: int=None, chunk: int = None, min_N: int=4):\n",
    "    if chunk_size:\n",
    "        # Read in a bit of data at a time. \n",
    "        gd, gf, _ = golden_hds[glst].read(bls=list(lstbin_data[glst].bls())[chunk*chunk_size:(chunk+1)*chunk_size], read_flags=True)\n",
    "    else:\n",
    "        gd = golden_data[glst]\n",
    "        gf = golden_flags[glst]\n",
    "    \n",
    "    # We do this in a baseline-loop, which may be slower than it could be\n",
    "    zscores = {}\n",
    "    flags = {}\n",
    "            \n",
    "    for bl in gf.bls():\n",
    "        # Ignore autos\n",
    "        if bl[0] == bl[1] or bl[2][0] != bl[2][1]:\n",
    "            continue\n",
    "            \n",
    "        flg = gf[bl].copy()\n",
    "        \n",
    "        this = np.zeros(flg.shape, dtype=complex)\n",
    "        for part in ['real', 'imag']:\n",
    "        \n",
    "            d = getattr(gd[bl], part).copy()\n",
    "\n",
    "            flg[np.isnan(d) | np.isinf(d)] = True\n",
    "\n",
    "            d[flg] *= np.nan\n",
    "            location = np.nanmedian(d, axis=0)\n",
    "            mad = np.nanmedian(np.abs(d - location), axis=0) * 1.482579\n",
    "\n",
    "            if part == \"real\":\n",
    "                this += (d - location)/mad\n",
    "            else:\n",
    "                this += 1j * (d - location)/mad\n",
    "            \n",
    "        # Apply min_N criterion\n",
    "        # the point of \"flagging\" here is just to be able to exclude \n",
    "        # these values from showing up in the computed fractions\n",
    "        # that are flagged specifically because of their Z-score.\n",
    "        ndays_binned = np.sum(~flg, axis=0)\n",
    "        flg[:, ndays_binned < min_N] = True\n",
    "\n",
    "        zscores[bl] = this\n",
    "        flags[bl] = flg\n",
    "\n",
    "    if RED_DATA:\n",
    "        return RedDataContainer(zscores, reds=reds), RedDataContainer(flags, reds=reds),\n",
    "    else:\n",
    "        return DataContainer(zscores), DataContainer(flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_clip_fraction_plot(thresholds, sigma_clip_fracs):\n",
    "    nglst = len(sigma_clip_fracs)\n",
    "    ncols = 3\n",
    "    nrows = nglst // ncols + 1\n",
    "    fig, axx = plt.subplots(nrows, 3, sharey=True, sharex=True, constrained_layout=True, figsize=(15, 2*nrows))\n",
    "\n",
    "    cdf = 2* norm().cdf(-np.array(thresholds))\n",
    "\n",
    "    ax = axx.flatten()\n",
    "\n",
    "    # get legend entries\n",
    "    for jdint, style in styles.items():\n",
    "        ax[0].plot([0], [np.nan], label=str(jdint), **style)\n",
    "            \n",
    "    for j, (glst, fracs) in enumerate(sigma_clip_fracs.items()):\n",
    "        ax[j].plot(thresholds, cdf, color='k', label='Gaussian Theory' if j==0 else None)\n",
    "        \n",
    "        ax[j].text(0.5, 0.85, f\"LST {glst*12/np.pi:.2f}\", fontsize=14, transform=ax[j].transAxes)\n",
    "\n",
    "        for i, jd in enumerate(golden_data[glst].times):\n",
    "            for part in ['real', 'imag']:\n",
    "                frac_cut = np.mean([fracs[bl][i] for bl in fracs.bls()], axis=0)\n",
    "                intjd = int(jd)\n",
    "                ax[j].plot(thresholds, frac_cut, alpha=0.5 if part=='imag' else 1.0, **styles[intjd])\n",
    "    for axxx in ax[j+1:]:\n",
    "        axxx.axis('off')\n",
    "        \n",
    "    fig.supxlabel(\"sigma clip threshold\")\n",
    "\n",
    "    fig.legend(loc='center', ncols=3, bbox_to_anchor=(0.85, 0.18), frameon=False)\n",
    "    \n",
    "    fig.supylabel(\"Fraction of Samples Clipped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_chunk_sizes(x):\n",
    "    return np.diff(np.where(np.concatenate(([x[0]], x[:-1] != x[1:], [True])))[0])[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigma_clip_stats(min_N: int = 4, thresholds=(4,), ret_zscores=True):\n",
    "    if not RED_DATA:\n",
    "        chunk_size=None  # read in all baselines at once, but only one file at a time.\n",
    "        nchunks = 1\n",
    "    else:\n",
    "        chunk_size=None\n",
    "        nchunks = 1\n",
    "    \n",
    "    flag_fracs = {}\n",
    "    contig_sizes = {}\n",
    "    all_zscores = {}\n",
    "    \n",
    "    for glst in GOLDEN_LSTs:\n",
    "        flag_frac_glst = {}\n",
    "        contig_size_glst = {}\n",
    "        \n",
    "        if chunk_size:\n",
    "            nchunks = len(lstbin_data[glst].bls()) // chunk_size + 1\n",
    "    \n",
    "        all_zscores[glst] = RedDataContainer({}, reds=reds) if RED_DATA else DataContainer({})\n",
    "        \n",
    "        for chunk in range(nchunks):\n",
    "            print(f\"Getting MAD Z-Scores for Baseline Chunk {chunk + 1} of {nchunks}\")\n",
    "            zscores, pre_flags = get_observed_zscores(glst, chunk_size=chunk_size, chunk=chunk, min_N=min_N)\n",
    "\n",
    "            for bl in zscores.bls():\n",
    "                flag_frac_glst[bl] = []\n",
    "                contig_size_glst[bl] = []\n",
    "                \n",
    "                for i, (z, flg) in enumerate(zip(zscores[bl], pre_flags[bl])):\n",
    "                    \n",
    "                    fracs = []\n",
    "                    contig_size = []\n",
    "                    for thresh in thresholds:\n",
    "                        clip_flags = (np.abs(z.real) > thresh) | (np.abs(z.imag) > thresh)\n",
    "                        fracs.append(np.sum(clip_flags & (~flg)) / clip_flags.size)\n",
    "                        ch = get_true_chunk_sizes(clip_flags)\n",
    "                        if np.any(np.isnan(~ch)):\n",
    "                            contig_size.append(ch.max())\n",
    "                        else:\n",
    "                            contig_size.append(0)\n",
    "                    \n",
    "                    flag_frac_glst[bl].append(fracs)\n",
    "                    contig_size_glst[bl].append(contig_size)\n",
    "                    \n",
    "                flag_frac_glst[bl] = np.array(flag_frac_glst[bl])\n",
    "                contig_size_glst[bl] = np.array(contig_size_glst[bl])\n",
    "                \n",
    "            if ret_zscores:\n",
    "                all_zscores[glst] += zscores\n",
    "                \n",
    "        flag_fracs[glst] = RedDataContainer(flag_frac_glst, reds=reds) if RED_DATA else DataContainer(flag_frac_glst)\n",
    "        contig_sizes[glst] = RedDataContainer(contig_size_glst, reds=reds) if RED_DATA else DataContainer(contig_size_glst)\n",
    "        \n",
    "    return flag_fracs, contig_sizes, all_zscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = (3.5, 4, 4.5, 5., 5.5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_fracs, contig_sizes, zscores_obs = get_sigma_clip_stats(\n",
    "    min_N = config['LSTBIN_OPTS'].get(\"sigma_clip_min_N\", 4),\n",
    "    thresholds=thresholds, \n",
    "    ret_zscores=RED_DATA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Sigma-Clipped Fraction As Function of Threshold, LST and Night"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the underlying data is Gaussian, the fraction sigma-clipped can be predicted by the CDF of the Gaussian function as a function of threshold. In reality, we expect the data to have more outliers than an actual Gaussian. In the plot below, we show the fraction of samples (across baselines and frequencies) that are flagged specifically due to sigma-clipping, as a function of the sigma-clipping threshold. We split the plots between different LST bins and different nights. The real part is shown as the full-colour lines, while the imaginary part is shown as the 50% transparent lines of the same style. The black line is the theoretical expectation, given an underlying Gaussian distribution across nights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_clip_fraction_plot(thresholds, flag_fracs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of most sigma-clipped LSTs, Nights and Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One concern is that we might be flagging out large fractions of particular antennas with sigma-clipping. In this case, it would be ideal to identify the actual issue in a previous step, rather than arbitrarily sigma-clipping them at the LST-binning step. Here, we find the most sigma-clipped baselines for any night/LST, and print all those that are sigma-clipped more than 30% (at 4-sigma). This is purely _sigma-clipped_ flags, where pre-flagged data is not counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ones = {}\n",
    "threshidx = thresholds.index(4)\n",
    "for glst, dc in flag_fracs.items():\n",
    "    for bl in dc.bls():\n",
    "        if bl[2][0] != bl[2][1]:\n",
    "            continue\n",
    "        for i, jd in enumerate(golden_data[glst].times):\n",
    "            if dc[bl][i, threshidx] > 0.3:\n",
    "                bad_ones[(glst*12/np.pi, int(jd), bl)] = dc[bl][i, threshidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (glst, jd, bl), frac in sorted(bad_ones.items(), key=lambda item: item[1])[-1:-100:-1]:\n",
    "    print(f\"LST {glst:5.2f} hr on night {jd} for bl {bl} had {frac*100:.1f}% sigma-clip flags\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclipped_ee = [len([x for key, x in bad_ones.items() if key[2][2]=='ee' and x > thresh]) for thresh in np.arange(0.3, 1, 0.1)]\n",
    "nclipped_nn = [len([x for key, x in bad_ones.items() if key[2][2]=='nn' and x > thresh]) for thresh in np.arange(0.3, 1, 0.1)]\n",
    "\n",
    "plt.bar(np.arange(0.3, 1., 0.1), nclipped_ee, width=0.03, color='C0', label='ee', align='edge')\n",
    "plt.bar(np.arange(0.3, 1., 0.1), nclipped_nn, width=-0.03, color='C1', label='nn', align='edge')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.title(\"Number of (LST, night, bl) combos flagged \\n more than a certain fraction of channels\")\n",
    "plt.xlabel(\"Flag Fraction (over frequency)\")\n",
    "plt.ylabel(\"Number of (LST, night, bl) combos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# antennas flagged\n",
    "if not RED_DATA:\n",
    "    antflags = {}\n",
    "    threshes = np.arange(0.3, 1, 0.1)\n",
    "    for (lst, jd, bl), frac in bad_ones.items():\n",
    "        a, b = utils.split_bl(bl)\n",
    "        if a not in antflags:\n",
    "            antflags[a] = np.zeros_like(threshes)\n",
    "        if b not in antflags:\n",
    "            antflags[b] = np.zeros_like(threshes)\n",
    "\n",
    "        antflags[a] += (frac >= threshes).astype(int)\n",
    "        antflags[b] += (frac >= threshes).astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Most-Sigma-Clipped Antennas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RED_DATA:\n",
    "    print(f\"Number of baseline-LST-night combos clipped more than given % across frequency, for given ant\")\n",
    "    print(\"==============================================================================================\")\n",
    "    print(\"Chans clipped: 30%     40%  50%  60%  70%  80%  90%\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    \n",
    "    for ant, fracs in sorted(antflags.items(), key=lambda item: item[1][0])[-1:-25:-1]:\n",
    "        print(f\"{ant[0]:>3}{ant[1][-1]}:          {fracs.astype(int)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ant-nights\n",
    "if not RED_DATA:\n",
    "    antnightflags = {}\n",
    "    threshes = np.arange(0.3, 1, 0.1)\n",
    "    for (lst, jd, bl), frac in bad_ones.items():\n",
    "        a, b = utils.split_bl(bl)\n",
    "        if (a, jd) not in antnightflags:\n",
    "            antnightflags[(a, jd)] = np.zeros_like(threshes)\n",
    "        if (b, jd) not in antnightflags:\n",
    "            antnightflags[(b, jd)] = np.zeros_like(threshes)\n",
    "\n",
    "        antnightflags[(a, jd)] += (frac >= threshes).astype(int)\n",
    "        antnightflags[(b, jd)] += (frac >= threshes).astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Most-Sigma-Clipped Antenna-Nights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RED_DATA:\n",
    "    print(f\"Number of baseline-LST combos clipped more than given % across frequency, for given night-ant\")\n",
    "    print(\"==============================================================================================\")\n",
    "    print(\"Chans clipped: 30%  40% 50% 60% 70% 80% 90%\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    \n",
    "    for (ant, jd), fracs in sorted(antnightflags.items(), key=lambda item: item[1][0])[-1:-25:-1]:\n",
    "        print(f\"{jd}-{ant[0]:>3}{ant[1][-1]}: {fracs.astype(int)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Counts of Contiguous Flagged Region Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also worry about when large contiguous chunks of frequency are flagged for a particular antenna. Here we plot the number of contiguous regions of a given size that are flagged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contig_sizes = [ctg[bl][0] for ctg in contig_sizes.values() for bl in ctg.bls()]\n",
    "\n",
    "plt.hist(all_contig_sizes, bins=np.arange(10, 1535, 10))\n",
    "\n",
    "# plt.scatter(cnt.keys(), cnt.values(), label='Sigma-clip Flags')\n",
    "# plt.scatter(cnt_nopreflags.keys(), cnt_nopreflags.values(), label=\"Not counting pre-flagged\", marker='x')\n",
    "\n",
    "# plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Number of contiguous flags (in frequency)\")\n",
    "plt.ylabel(\"Number of occurences\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T19:37:29.396496Z",
     "start_time": "2021-02-16T19:37:28.585720Z"
    }
   },
   "outputs": [],
   "source": [
    "import hera_cal\n",
    "import pyuvdata\n",
    "print('hera_cal version: ', hera_cal.__version__)\n",
    "print('pyuvdata version: ', pyuvdata.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h6c",
   "language": "python",
   "name": "h6c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "338.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
