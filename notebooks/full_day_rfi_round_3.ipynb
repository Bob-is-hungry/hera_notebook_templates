{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f07f44-3034-40c5-8df0-94d9bb93b1e9",
   "metadata": {},
   "source": [
    "# Third Round of Full Day RFI Flagging Using 2D-Filtered SNRs\n",
    "\n",
    "\n",
    "**by Josh Dillon**, last updated May 13, 2025\n",
    "\n",
    "This notebook brings together the results of [single-baseline 2D DPSS filtering notebook](https://github.com/HERA-Team/hera_notebook_templates/blob/master/notebooks/single_baseline_2D_filtered_SNRs.ipynb) to make a set of flagging decisions prior to inpainting. This approach is iterative, and very similar to [Round 2 flagging](https://github.com/HERA-Team/hera_notebook_templates/blob/master/notebooks/full_day_rfi_round_2.ipynb), though it includes special treatment of TV allocations ([see HERA Memo #82 for more details](https://reionization.org/wp-content/uploads/2013/03/HERA082_TV_Info.pdf)). \n",
    "\n",
    "Here's a set of links to skip to particular figures and tables:\n",
    "# [• Figure 1: Waterfall of Maximum z-Score of Either Polarization Before Round 3 Flagging](#Figure-1:-Waterfall-of-Maximum-z-Score-of-Either-Polarization-Before-Round-3-Flagging)\n",
    "# [• Figure 2: Histogram of z-Scores](#Figure-2:-Histogram-of-z-Scores)\n",
    "# [• Figure 3: Waterfall of Maximum z-Score of Either Polarization After Round 3 Flagging](#Figure-3:-Waterfall-of-Maximum-z-Score-of-Either-Polarization-After-Round-3-Flagging)\n",
    "# [• Figure 4: Summary of Flags Before and After Round 3 Flagging](#Figure-4:-Summary-of-Flags-Before-and-After-Round-3-Flagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac70e3e-0b21-4aad-8e3a-73f8fc0f4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tstart = time.time()\n",
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71e642-1255-44b7-9b90-f58de15c5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "import h5py\n",
    "import hdf5plugin  # REQUIRED to have the compression plugins available\n",
    "import numpy as np\n",
    "import yaml\n",
    "import glob\n",
    "import re\n",
    "import matplotlib\n",
    "from scipy.signal import convolve, convolve2d\n",
    "from pyuvdata import UVFlag\n",
    "from hera_qm import xrfi\n",
    "from hera_cal import io, flag_utils\n",
    "from hera_filters import dspec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "_ = np.seterr(all='ignore')  # get rid of red warnings\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c19e2e-fb33-4c25-bb7a-94d52ae73eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RED_AVG_FILE = os.environ.get(\"RED_AVG_FILE\", None)\n",
    "# RED_AVG_FILE = '/lustre/aoc/projects/hera/jsdillon/H6C/IDR3/2459861/zen.2459861.25364.sum.smooth_calibrated.red_avg.uvh5'\n",
    "\n",
    "CORNER_TURN_MAP_YAML = os.environ.get(\"CORNER_TURN_MAP_YAML\", \n",
    "                                        os.path.join(os.path.dirname(RED_AVG_FILE), \"single_baseline_files/corner_turn_map.yaml\"))\n",
    "SNR_SUFFIX = os.environ.get(\"SNR_SUFFIX\", \".2Dfilt_SNR.uvh5\")\n",
    "OUTFILE = os.environ.get(\"OUTFILE\", None)\n",
    "if OUTFILE is None:\n",
    "    jdstr = [s for s in os.path.basename(RED_AVG_FILE).split('.') if s.isnumeric()][0]\n",
    "    OUTFILE = os.path.basename(RED_AVG_FILE).split(jdstr)[0] + jdstr + '.flag_waterfall_round_3.h5'\n",
    "    OUTFILE = os.path.join(os.path.dirname(CORNER_TURN_MAP_YAML), OUTFILE)\n",
    "\n",
    "MIN_SAMP_FRAC = float(os.environ.get(\"MIN_SAMP_FRAC\", .15))\n",
    "FM_LOW_FREQ = float(os.environ.get(\"FM_LOW_FREQ\", 87.5)) # in MHz\n",
    "FM_HIGH_FREQ = float(os.environ.get(\"FM_HIGH_FREQ\", 108.0)) # in MHz\n",
    "\n",
    "Z_THRESH = float(os.environ.get(\"Z_THRESH\", 4))\n",
    "WS_Z_THRESH = float(os.environ.get(\"WS_Z_THRESH\", 2))\n",
    "AVG_Z_THRESH = float(os.environ.get(\"AVG_Z_THRESH\", 1))\n",
    "MAX_FREQ_FLAG_FRAC = float(os.environ.get(\"MAX_FREQ_FLAG_FRAC\", .25))\n",
    "MAX_TIME_FLAG_FRAC = float(os.environ.get(\"MAX_TIME_FLAG_FRAC\", .25))\n",
    "\n",
    "TV_CHAN_EDGES = os.environ.get(\"TV_CHAN_EDGES\", \"174,182,190,198,206,214,222,230,238,246,254\")\n",
    "\n",
    "FREQ_CONV_SIZE  = float(os.environ.get(\"FREQ_CONV_SIZE\", 8.0)) # in MHz\n",
    "\n",
    "for setting in ['RED_AVG_FILE', 'CORNER_TURN_MAP_YAML', 'SNR_SUFFIX', 'TV_CHAN_EDGES', 'OUTFILE']:\n",
    "    print(f'{setting} = \"{eval(setting)}\"')\n",
    "for setting in ['MIN_SAMP_FRAC', 'FM_LOW_FREQ', 'FM_HIGH_FREQ', 'Z_THRESH', 'WS_Z_THRESH',\n",
    "                'AVG_Z_THRESH', 'MAX_FREQ_FLAG_FRAC', 'MAX_TIME_FLAG_FRAC', 'FREQ_CONV_SIZE']:\n",
    "    print(f'{setting} = {eval(setting)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636df35-7f86-4d53-9be2-6c8e87909f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CORNER_TURN_MAP_YAML, 'r') as file:\n",
    "    corner_turn_map = yaml.unsafe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ceecb-5f7f-40a7-92f1-1b85e29e3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_snr_files = [snr_file.replace('.uvh5', SNR_SUFFIX) \n",
    "                 for snr_files in corner_turn_map['files_to_outfiles_map'].values() \n",
    "                 for snr_file in snr_files]\n",
    "extant_snr_files = [snr_file for snr_file in all_snr_files if os.path.exists(snr_file)]\n",
    "print(f'Found {len(extant_snr_files)} SNR files, starting with {extant_snr_files[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d40c3-d851-4619-9dc7-dca8d08f29c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get autocorrelations\n",
    "all_outfiles = [outfile for outfiles in corner_turn_map['files_to_outfiles_map'].values() for outfile in outfiles]\n",
    "for outfile in all_outfiles:\n",
    "    match = re.search(r'\\.(\\d+)_(\\d+)\\.', os.path.basename(outfile))\n",
    "    if match and match.group(1) == match.group(2):\n",
    "        hd_autos = io.HERAData(outfile)\n",
    "        _, _, auto_nsamples = hd_autos.read(polarizations=['ee', 'nn'])\n",
    "        break\n",
    "\n",
    "med_auto_nsamples = {pol: np.median(auto_nsamples[0,0,pol]) for pol in ['ee', 'nn']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2997a1-2eca-4932-a4f2-b7d25810981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define slices for TV allocations\n",
    "tv_edges = [float(edge) for edge in TV_CHAN_EDGES.split(',')]\n",
    "tv_slices = []\n",
    "for i in range(len(tv_edges) - 1):\n",
    "    chans_in_band = np.argwhere((hd_autos.freqs / 1e6 > tv_edges[i]) & (hd_autos.freqs / 1e6 < tv_edges[i+1]))\n",
    "    if len(chans_in_band) > 0:\n",
    "        tv_slices.append(slice(np.min(chans_in_band), np.max(chans_in_band) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff6f21-cde7-4733-9b1f-e260e4e61c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up SNRs, counts, and nsamples\n",
    "hd = io.HERADataFastReader(extant_snr_files[0])\n",
    "abs_SNR_sums = {}\n",
    "abs_SNR_counts = {}\n",
    "abs_SNR_med_nsamples = {}\n",
    "\n",
    "# for snr_file in tqdm(extant_snr_files[0:5]):\n",
    "for snr_file in extant_snr_files:\n",
    "    hd = io.HERADataFastReader(snr_file)\n",
    "    data, flags, nsamples = hd.read()\n",
    "    for bl in data:\n",
    "        abs_SNR_sums[bl] = np.where(flags[bl], 0, np.abs(data[bl]))\n",
    "        abs_SNR_counts[bl] = np.where(flags[bl], 0, 1)\n",
    "        abs_SNR_med_nsamples[bl] = np.median(nsamples[bl][~flags[bl]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0739310-982c-4301-b6f1-f812e0bb5e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine SNRs incoherently, excluding those with too few samples\n",
    "abs_SNR_sum = {pol: np.zeros((len(hd.times), len(hd.freqs)), dtype=float) for pol in hd.pols}\n",
    "abs_SNR_count = {pol: np.zeros((len(hd.times), len(hd.freqs)), dtype=float) for pol in hd.pols}\n",
    "bls_used = []\n",
    "for bl in abs_SNR_sums:\n",
    "    if np.median(abs_SNR_med_nsamples[bl]) > MIN_SAMP_FRAC * med_auto_nsamples[bl[2]]:\n",
    "        if np.linalg.norm(hd.antpos[bl[0]] - hd.antpos[bl[1]]) > 1:\n",
    "            bls_used.append(bl)\n",
    "            abs_SNR_sum[bl[2]] += abs_SNR_sums[bl]\n",
    "            abs_SNR_count[bl[2]] += abs_SNR_counts[bl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee069fba-4abf-49dd-b330-f198bfa77ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert SNRs to a z-score\n",
    "zscore = {}\n",
    "for pol in abs_SNR_sum.keys():\n",
    "    predicted_mean = 1.0\n",
    "    sigma = predicted_mean * np.sqrt(2 / np.pi)\n",
    "    variance_expected = (4 - np.pi) / 2 * sigma**2 / abs_SNR_count[pol]\n",
    "    zscore[pol] = (abs_SNR_sum[pol] / abs_SNR_count[pol] - predicted_mean) / variance_expected**.5\n",
    "    zscore[pol] = np.where(abs_SNR_count[pol] == 0, np.nan, zscore[pol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6acacde-9389-4e88-846a-c2e4b6d3d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recenter z-scores above and below FM and per-polarization\n",
    "_, (low_band, high_band) = flag_utils.get_minimal_slices(np.any(~np.isfinite(list(zscore.values())), axis=0), \n",
    "                                                         freqs=data.freqs, freq_cuts=[FM_LOW_FREQ / 2 + FM_HIGH_FREQ / 2])\n",
    "for pol in zscore:\n",
    "    for band in [low_band, high_band]:\n",
    "        zscore[pol][:, band] -= np.nanmedian(zscore[pol][:, band])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928fe951-e15a-4592-8478-b118b9b0c859",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccfd03c-972a-468d-8138-ca2be053fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_max_z_score(zscore, flags=None, vmin=-5, vmax=5):\n",
    "    if flags is None:\n",
    "        flags = np.any(~np.isfinite(list(zscore.values())), axis=0)\n",
    "    plt.figure(figsize=(14,10), dpi=300)\n",
    "    extent = [data.freqs[0] / 1e6, data.freqs[-1] / 1e6, \n",
    "              data.times[-1] - int(data.times[0]), data.times[0] - int(data.times[0])]\n",
    "    \n",
    "    plt.imshow(np.where(flags, np.nan, np.nanmax([zscore['ee'], zscore['nn']], axis=0)), aspect='auto', \n",
    "               cmap='coolwarm', interpolation='none', vmin=vmin, vmax=vmax, extent=extent)\n",
    "    plt.colorbar(location='top', label='Max z-score of either polarization', extend='both', aspect=40, pad=.02)\n",
    "    plt.xlabel('Frequency (MHz)')\n",
    "    plt.ylabel(f'JD - {int(data.times[0])}')\n",
    "    plt.tight_layout()\n",
    "    for freq in tv_edges:\n",
    "        if freq < data.freqs[-1] * 1e-6:\n",
    "            plt.axvline(freq, lw=.5, ls='--', color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22f659-0543-4b3e-bb21-da478b4bd7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram():\n",
    "    plt.figure(figsize=(14,4), dpi=100)\n",
    "    bins = np.arange(-50, 100, .1)\n",
    "    hist_ee = plt.hist(np.ravel(zscore['ee']), bins=bins, density=True, label='ee-polarized z-scores', alpha=.5)\n",
    "    hist_nn = plt.hist(np.ravel(zscore['nn']), bins=bins, density=True, label='nn-polarized z-scores', alpha=.5)\n",
    "    plt.plot(bins, (2*np.pi)**-.5 * np.exp(-bins**2 / 2), 'k:', label='Gaussian approximate\\nnoise-only distribution')\n",
    "    plt.axvline(WS_Z_THRESH, c='r', ls='--', label='Watershed z-score')\n",
    "    plt.axvline(Z_THRESH, c='r', ls='-', label='Threshold z-score')    \n",
    "    plt.yscale('log')\n",
    "    all_densities = np.concatenate([hist_ee[0][hist_ee[0] > 0], hist_nn[0][hist_nn[0] > 0]]) \n",
    "    plt.ylim(np.min(all_densities) / 2, np.max(all_densities) * 2)\n",
    "    plt.xlim([-50, 100])\n",
    "    plt.legend()\n",
    "    plt.xlabel('z-score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26015188-fbf9-4157-b53d-c42fc661b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_flagging(flags):\n",
    "    plt.figure(figsize=(14,10), dpi=200)\n",
    "    cmap = matplotlib.colors.ListedColormap(((0, 0, 0),) + matplotlib.cm.get_cmap(\"Set2\").colors[0:2])\n",
    "    extent = [data.freqs[0] / 1e6, data.freqs[-1] / 1e6, \n",
    "              data.times[-1] - int(data.times[0]), data.times[0] - int(data.times[0])]    \n",
    "    plt.imshow(np.where(np.any(~np.isfinite(list(zscore.values())), axis=0), 1, np.where(flags, 2, 0)), \n",
    "               aspect='auto', cmap=cmap, interpolation='none', extent=extent)\n",
    "    plt.clim([-.5, 2.5])\n",
    "    cbar = plt.colorbar(location='top', aspect=40, pad=.02)\n",
    "    cbar.set_ticks([0, 1, 2])\n",
    "    cbar.set_ticklabels(['Unflagged', 'Flagged After Round 2', 'Flagged After Round 3'])\n",
    "    plt.xlabel('Frequency (MHz)')\n",
    "    plt.ylabel(f'JD - {int(data.times[0])}')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf629c-0c9d-476b-8e78-dfe45263df7c",
   "metadata": {},
   "source": [
    "# Figure 1: Waterfall of Maximum z-Score of Either Polarization Before Round 3 Flagging\n",
    "\n",
    "This figure shows the worse (higher z-score) of the two polarizations. Dotted lines in the high band show TV allocations, which recieve special treatment. Large positive excursions are problem and likely need flagging. note that below and FM are handled separately and may have different levels of post-flag filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eeb859-b44b-41ba-a8c2-e28e2c442b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_z_score(zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab511e22-2e1e-401d-b7f1-a925496a154d",
   "metadata": {},
   "source": [
    "# Figure 2: Histogram of z-Scores\n",
    "\n",
    "Shows a comparison of the histogram of z-scores to a Gaussian approximation of what one might expect from thermal noise. Without filtering, the actual distribution is a weighted sum of Rayleigh distributions. Filtering further complicates this. To make the z-scores more reliable, a single per-polarization and per-band median is subtracted from each waterfall, which allows us to flag low-level outliers with more confidence. Any points beyond the solid red line are flagged. Any points neighboring a flag beyond the dashed red line are also flagged. Finally, flagging is performed for low-level outliers on whole times or channels, in TV allocations, and other compact regions in frequency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbcdf8c-a23f-43f5-8ea7-d31e88ebc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b3011-b0bd-465f-9c70-2dc7098355ab",
   "metadata": {},
   "source": [
    "## Flagging functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78f057-26c9-45ee-b4a7-9d428e43ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteratively_flag_on_averaged_zscore(flags, zscore, avg_func=np.nanmean, avg_z_thresh=AVG_Z_THRESH, verbose=True):\n",
    "    '''Flag whole integrations or channels based on average z-score. This is done\n",
    "    iteratively to prevent bad times affecting channel averages or vice versa.'''\n",
    "\n",
    "    _, (low_band, high_band) = flag_utils.get_minimal_slices(flags, freqs=data.freqs, freq_cuts=[(FM_LOW_FREQ + FM_HIGH_FREQ) * .5e6])\n",
    "    flagged_chan_count = 0\n",
    "    flagged_int_count = {low_band: 0, high_band: 0}\n",
    "    for band in (low_band, high_band):\n",
    "        while True:\n",
    "            zspec = avg_func(np.where(flags, np.nan, zscore)[:, band], axis=0)\n",
    "            ztseries = avg_func(np.where(flags, np.nan, zscore)[:, band], axis=1)\n",
    "    \n",
    "            if (np.nanmax(zspec) < avg_z_thresh) and (np.nanmax(ztseries) < avg_z_thresh):\n",
    "                break\n",
    "    \n",
    "            if np.nanmax(zspec) >= np.nanmax(ztseries):\n",
    "                flagged_chan_count += np.sum((zspec >= np.nanmax(ztseries)) & (zspec >= avg_z_thresh))\n",
    "                flags[:, band][:, (zspec >= np.nanmax(ztseries)) & (zspec >= avg_z_thresh)] = True\n",
    "            else:\n",
    "                flagged_int_count[band] += np.sum((ztseries >= np.nanmax(zspec)) & (ztseries >= avg_z_thresh))\n",
    "                flags[(ztseries >= np.nanmax(zspec)) & (ztseries >= avg_z_thresh), band] = True\n",
    "\n",
    "    ztseries_low = avg_func(np.where(flags, np.nan, zscore)[:, low_band], axis=1)\n",
    "    flags[(ztseries_low > avg_z_thresh) & np.all(flags[:, high_band], axis=1), low_band] = True\n",
    "    \n",
    "    if verbose:\n",
    "        if (flagged_int_count[low_band] > 0) or (flagged_int_count[high_band] > 0) or (flagged_chan_count > 0):\n",
    "            print(f'\\tFlagging an additional {flagged_int_count[low_band]} low-band integrations, '\n",
    "                  f'{flagged_int_count[high_band]} high-band integrations, and {flagged_chan_count} channels.')\n",
    "\n",
    "def impose_max_chan_flag_frac(flags, max_flag_frac=MAX_FREQ_FLAG_FRAC, verbose=True):\n",
    "    '''Flag channels already flagged more than max_flag_frac (excluding completely flagged times).'''\n",
    "    _, (low_band, high_band) = flag_utils.get_minimal_slices(flags, freqs=data.freqs, freq_cuts=[(FM_LOW_FREQ + FM_HIGH_FREQ) * .5e6])\n",
    "    for band in [low_band, high_band]:\n",
    "        unflagged_times = ~np.all(flags[:, band], axis=1)\n",
    "        frequently_flagged_chans =  np.mean(flags[unflagged_times, band], axis=0) >= max_flag_frac\n",
    "        if verbose:\n",
    "            flag_diff_count = np.sum(frequently_flagged_chans) - np.sum(np.all(flags[:, band], axis=0))\n",
    "            if flag_diff_count > 0:\n",
    "                print(f'\\tFlagging {flag_diff_count} channels previously flagged {max_flag_frac:.2%} or more.')        \n",
    "        flags[:, band][:, frequently_flagged_chans] = True\n",
    "        \n",
    "def impose_max_time_flag_frac(flags, max_flag_frac=MAX_TIME_FLAG_FRAC, verbose=True):\n",
    "    '''Flag times already flagged more than max_flag_frac (excluding completely flagged channels).'''\n",
    "    _, (low_band, high_band) = flag_utils.get_minimal_slices(flags, freqs=data.freqs, freq_cuts=[(FM_LOW_FREQ + FM_HIGH_FREQ) * .5e6])\n",
    "    for name, band in zip(['low', 'high'], [low_band, high_band]):\n",
    "        unflagged_chans = ~np.all(flags[:, band], axis=0)\n",
    "        frequently_flagged_times =  np.mean(flags[:, band][:, unflagged_chans], axis=1) >= max_flag_frac\n",
    "        if verbose:\n",
    "            flag_diff_count = np.sum(frequently_flagged_times) - np.sum(np.all(flags[:, band], axis=1))\n",
    "            if flag_diff_count > 0:\n",
    "                print(f'\\tFlagging {flag_diff_count} {name}-band times previously flagged {max_flag_frac:.2%} or more.')\n",
    "        flags[frequently_flagged_times, band] = True\n",
    "\n",
    "def flag_tv(flags, zscore, tv_thresh=AVG_Z_THRESH, egregious_thresh=(2 * Z_THRESH)):\n",
    "    '''Flag single-time TV allocations with average zscores above tv_thresh, excluding particularly bad channels.'''\n",
    "    for pol in zscore:\n",
    "        for tvs in tv_slices:\n",
    "            for tind in range(zscore[pol].shape[0]):\n",
    "                if np.nanmean(zscore[pol][tind, tvs]) > tv_thresh:\n",
    "                    egregious_outliers = zscore[pol][tind, tvs] > egregious_thresh\n",
    "                    if np.nanmean(zscore[pol][tind, tvs][~egregious_outliers]) > tv_thresh:\n",
    "                        # even without the worst outliers, it still looks like there's TV in this whole allocation\n",
    "                        flags[tind, tvs] = True\n",
    "                    else:\n",
    "                        # just flag the most egregious outliers\n",
    "                        flags[tind, tvs] |= egregious_outliers\n",
    "\n",
    "def watershed_flag(flags, zscore, ws_z_thresh=WS_Z_THRESH):\n",
    "    '''Wrapper around xrfi._ws_flag_waterfall to be performed separately above and below FM.'''\n",
    "    while True:        \n",
    "        nflags = np.sum(flags)\n",
    "        _, (low_band, high_band) = flag_utils.get_minimal_slices(flags, freqs=data.freqs, freq_cuts=[(FM_LOW_FREQ + FM_HIGH_FREQ) * .5e6])\n",
    "        for band in [low_band, high_band]:\n",
    "            for pol in ['ee', 'nn']:\n",
    "                flags[:, band] |= xrfi._ws_flag_waterfall(zscore[pol][:, band], flags[:, band], ws_z_thresh)\n",
    "        if np.sum(flags) == nflags:\n",
    "            break\n",
    "\n",
    "def iterative_freq_conv_flagging(flags, zscore, conv_size=FREQ_CONV_SIZE, one_chan_thresh=Z_THRESH, full_kernel_thresh=AVG_Z_THRESH):\n",
    "    '''Looks for streteches of increasing size that fit a decreasing threshold. At conv_size (in MHz), it flags \n",
    "    stretches with average z-score above full_kernel_thresh. At one pixel, it uses one_chan_thresh.\n",
    "    In between, it interpolates logarithmically.'''\n",
    "    _, (low_band, high_band) = flag_utils.get_minimal_slices(flags, freqs=data.freqs, freq_cuts=[(FM_LOW_FREQ + FM_HIGH_FREQ) * .5e6])\n",
    "    df_MHz = np.median(np.diff(data.freqs)) / 1e6\n",
    "    widths = np.array([int(w) + 1 for w in 2**np.arange(1, np.ceil(np.log2(FREQ_CONV_SIZE / df_MHz) + np.finfo(float).eps))])\n",
    "    \n",
    "    # prevent any widths from being so big that they mix high and low bands\n",
    "    max_width = (high_band.start - low_band.stop) * 2\n",
    "    widths[widths > max_width] = max_width\n",
    "    widths = np.unique(widths)\n",
    "\n",
    "    # Create cuts that get more strict as the kernel gets bigger\n",
    "    cuts = one_chan_thresh * (full_kernel_thresh / one_chan_thresh)**((widths - 1) / (conv_size / df_MHz - 1))\n",
    "\n",
    "    for width, cut in zip(widths, cuts):\n",
    "        result = {}\n",
    "        for pol in zscore.keys():\n",
    "            kernel = np.ones((1, int(width)), dtype=float)\n",
    "            mask = ~(np.isnan(zscore[pol]) | flags)\n",
    "            filled_data = np.where(mask, zscore[pol], 0.0)\n",
    "            conv_data = convolve2d(filled_data, kernel, mode='same')\n",
    "            conv_mask = convolve2d(mask.astype(float), kernel, mode='same')\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                result[pol] = conv_data / conv_mask\n",
    "\n",
    "        for band in [low_band, high_band]:\n",
    "            above_cut = np.any([result[pol][:, band] > cut for pol in result.keys()], axis=0)\n",
    "            flags[:, band] |= (convolve2d(above_cut.astype(float), kernel, mode='same') > 0)\n",
    "        \n",
    "        print(f'{np.mean(flags):.3%} of waterfall flagged after {width}-channel convolution-based flagging with z-scores above {cut:.3f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c14398-13fa-435f-ad3d-1c59344ab880",
   "metadata": {},
   "source": [
    "## Main Flagging Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5fa42-a286-418f-ab0a-ad7bf712854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = np.any(~np.isfinite(list(zscore.values())), axis=0)\n",
    "_, (low_band, high_band) = flag_utils.get_minimal_slices(flags, freqs=data.freqs, freq_cuts=[100e6])\n",
    "print(f'{np.mean(flags):.3%} of waterfall flagged to start.')\n",
    "\n",
    "# flag bad TV allocations\n",
    "flag_tv(flags, zscore)\n",
    "print(f'{np.mean(flags):.3%} of waterfall flagged after TV channel cuts.')\n",
    "\n",
    "# flag whole integrations or channels using outliers in median\n",
    "while True:\n",
    "    nflags = np.sum(flags)\n",
    "    for pol in ['ee', 'nn']:    \n",
    "        iteratively_flag_on_averaged_zscore(flags, zscore[pol], avg_func=np.nanmedian, avg_z_thresh=AVG_Z_THRESH, verbose=True)\n",
    "        impose_max_chan_flag_frac(flags, max_flag_frac=MAX_FREQ_FLAG_FRAC, verbose=True)\n",
    "        impose_max_time_flag_frac(flags, max_flag_frac=MAX_TIME_FLAG_FRAC, verbose=True)\n",
    "    if np.sum(flags) == nflags:\n",
    "        break  \n",
    "print(f'{np.mean(flags):.3%} of waterfall flagged after flagging whole times and channels with median z > {AVG_Z_THRESH}.')\n",
    "\n",
    "# flag largest outliers\n",
    "_, (low_band, high_band) = flag_utils.get_minimal_slices(flags, freqs=data.freqs, freq_cuts=[(FM_LOW_FREQ + FM_HIGH_FREQ) * .5e6])\n",
    "for band in [low_band, high_band]:\n",
    "    for pol in ['ee', 'nn']:\n",
    "        flags[:, band] |= (zscore[pol][:, band] > Z_THRESH) \n",
    "print(f'{np.mean(flags):.3%} of waterfall flagged after flagging z > {Z_THRESH} outliers.')\n",
    "\n",
    "# watershed flagging\n",
    "watershed_flag(flags, zscore, ws_z_thresh=WS_Z_THRESH)\n",
    "print(f'{np.mean(flags):.3%} of waterfall flagged after watershed flagging on z > {WS_Z_THRESH} neighbors of prior flags.')\n",
    "\n",
    "# iterative frequency-convolved flagging\n",
    "iterative_freq_conv_flagging(flags, zscore, conv_size=FREQ_CONV_SIZE, one_chan_thresh=Z_THRESH, full_kernel_thresh=AVG_Z_THRESH)\n",
    "print(f'{np.mean(flags):.3%} of waterfall flagged after channel convolution flagging.')\n",
    "\n",
    "# watershed flagging\n",
    "watershed_flag(flags, zscore, ws_z_thresh=WS_Z_THRESH)\n",
    "print(f'{np.mean(flags):.3%} of waterfall flagged after watershed flagging again on z > {WS_Z_THRESH} neighbors of prior flags.')\n",
    "\n",
    "# flag whole integrations or channels using outliers in mean\n",
    "while True:\n",
    "    nflags = np.sum(flags)\n",
    "    for pol in ['ee', 'nn']:    \n",
    "        iteratively_flag_on_averaged_zscore(flags, zscore[pol], avg_func=np.nanmean, avg_z_thresh=AVG_Z_THRESH, verbose=True)\n",
    "        impose_max_chan_flag_frac(flags, max_flag_frac=MAX_FREQ_FLAG_FRAC, verbose=True)\n",
    "        impose_max_time_flag_frac(flags, max_flag_frac=MAX_TIME_FLAG_FRAC, verbose=True)\n",
    "    if np.sum(flags) == nflags:\n",
    "        break  \n",
    "print(f'{np.mean(flags):.3%} of waterfall flagged after flagging whole times and channels with average z > {AVG_Z_THRESH}.')\n",
    "\n",
    "# watershed flagging again\n",
    "watershed_flag(flags, zscore, ws_z_thresh=WS_Z_THRESH)\n",
    "print(f'{np.mean(flags):.3%} of waterfall flagged after watershed flagging one last time on z > {WS_Z_THRESH} neighbors of prior flags.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d65a1a-0418-4ac4-a538-0d9b8c1ed25b",
   "metadata": {},
   "source": [
    "# Figure 3: Waterfall of Maximum z-Score of Either Polarization After Round 3 Flagging\n",
    "\n",
    "Same as [Figure 1](#Figure-1:-Waterfall-of-Maximum-z-Score-of-Either-Polarization-Before-Round-3-Flagging) above, but now with additional flagging from this round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316bd637-0f5e-481d-a35f-9c73c306cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_z_score(zscore, flags=flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0039065f-e179-4bb2-aba5-c44c4dc9ab6e",
   "metadata": {},
   "source": [
    "# Figure 4: Summary of Flags Before and After Round 3 Flagging\n",
    "\n",
    "This plot shows which times and frequencies were flagged before and after this notebook. It is directly comparable to Figure 5 of the first round [full_day_rfi](https://github.com/HERA-Team/hera_notebook_templates/blob/master/notebooks/full_day_rfi.ipynb) notebook, as well as Figure 5 of the [full_day_rfi_round_2](https://github.com/HERA-Team/hera_notebook_templates/blob/master/notebooks/full_day_rfi_round_2.ipynb) notebook.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cfd806-de21-491f-b2a5-52b88144dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_flagging(flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d33f55-c378-4618-882d-8c63f5ed409c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T19:50:30.755937Z",
     "iopub.status.busy": "2025-03-21T19:50:30.755681Z",
     "iopub.status.idle": "2025-03-21T19:50:31.657022Z",
     "shell.execute_reply": "2025-03-21T19:50:31.656493Z",
     "shell.execute_reply.started": "2025-03-21T19:50:30.755919Z"
    }
   },
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d631e3a-c32b-4519-ba4f-9d6d627a7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "uvf = UVFlag(hd_autos, mode='flag', waterfall=True)\n",
    "for polind in range(uvf.flag_array.shape[2]):\n",
    "    uvf.flag_array[:, :, polind] = flags\n",
    "\n",
    "uvf.write(OUTFILE, clobber=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a87c7-e19f-4a45-9253-b58031f90fb5",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a488d-328f-4c52-8d85-a8af4be5e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in ['hera_cal', 'hera_qm', 'hera_filters', 'hera_notebook_templates', 'pyuvdata']:\n",
    "    exec(f'from {repo} import __version__')\n",
    "    print(f'{repo}: {__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da168374-4b97-45cb-a7cf-59fc648f3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished execution in {(time.time() - tstart) / 60:.2f} minutes.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
